{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5484ece7-c8bd-4374-b90c-8f84be4a05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "villes=[\"Mont Saint Michel\",\n",
    "\"St Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Chateau du Haut Koenigsbourg\",\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"Gorges du Verdon\",\n",
    "\"Bormes les Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix en Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues Mortes\",\n",
    "\"Saintes Maries de la mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67069493-76d3-4b33-8de5-fdc97373c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import os\n",
    "import logging\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.http import Request\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('villes.csv')\n",
    "df.head()\n",
    "villes=df.iloc[:,0].to_list()\n",
    "\n",
    "class BookingSpiderTest(scrapy.Spider):\n",
    "\n",
    "    locs = villes\n",
    "    name = \"booking\"\n",
    "    init_url = dict()\n",
    "    start_urls = [\"https://www.booking.com/\"]\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        for loc in self.locs:\n",
    "\n",
    "            yield scrapy.FormRequest.from_response(\n",
    "                response,\n",
    "                formdata={'ss': loc},\n",
    "                callback=self.after_search ,\n",
    "                cb_kwargs = {'location':loc, 'page_no':0}\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def after_search(self, response, location, page_no):\n",
    "\n",
    "        print(location, page_no, end=' ')\n",
    "        if page_no == 0:\n",
    "            self.init_url[location] = response.url\n",
    "\n",
    "        containers = response.css('div.a826ba81c4.fe821aea6c.fa2f36ad22.afd256fc79.d08f526e0d.ed11e24d01.da89aeb942')\n",
    "\n",
    "        for container in containers:\n",
    "            \n",
    "            try:\n",
    "                name = container.css('div.fcab3ed991.a23c043802::text').get()\n",
    "            except:\n",
    "                print(f\"Probleme avec le nom de l'hotel à {location} \")\n",
    "                name = None\n",
    "\n",
    "            try:\n",
    "                url = container.css('a.e13098a59f').attrib['href']\n",
    "            except:\n",
    "                print(f\"Probleme avec l'url de {name} à {location}\")\n",
    "                url = None\n",
    "            try:\n",
    "                description = container.css('div.d8eab2cf7f::text').get()\n",
    "            except:\n",
    "                print(f\"Probleme avec la description de {name} à {location}\")\n",
    "                description = None\n",
    "            try:\n",
    "                score = float(container.css('div.b5cd09854e.d10a6220b4::text').get())\n",
    "            except:\n",
    "                score = None\n",
    "                print(f\"Probleme avec le score de {name} à {location}\")\n",
    "\n",
    "            dic= {'location' : location,\n",
    "                   'url' : url,\n",
    "                   'name' : name,\n",
    "                   'score' : score,\n",
    "                   'description' : description}\n",
    "            try:\n",
    "                yield response.follow(url=url, callback=self.parse_hotel, cb_kwargs = {'dic':dic})\n",
    "            except:\n",
    "                print(f'\\n getting to {name}, {location} webpage did not work')\n",
    "                dic['lat']=None\n",
    "                dic['lon']=None\n",
    "                yield dic\n",
    "            \n",
    "            \n",
    "        if page_no<=1:\n",
    "            next_page = self.init_url[location]+\"&offset=\"+str(25*(page_no+1))\n",
    "            yield  response.follow(next_page, callback=self.after_search, cb_kwargs = {'location':location, 'page_no':page_no+1})\n",
    "        \n",
    "        \n",
    "    def parse_hotel(self, response, dic): #uniquement pour récupérer les coordonnées GPS\n",
    "        try:\n",
    "            ll = response.css('a#hotel_sidebar_static_map').attrib['data-atlas-latlng']\n",
    "        except:\n",
    "            print(\"mais pas réussi à récupérer le selecteur\")\n",
    "            dic['lat']=None\n",
    "            dic['lon']=None\n",
    "            yield dic\n",
    "            return None\n",
    "        try:\n",
    "            ll = ll.split(',')\n",
    "            dic['lat'] = float(ll[0])\n",
    "            dic['lon'] = float(ll[1])\n",
    "            yield dic\n",
    "        except:\n",
    "            print(\"mais pas réussi à séparer latitude et longitude\")\n",
    "            dic['lat']=None\n",
    "            dic['lon']=None\n",
    "            yield dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59687527-fc27-4c49-a932-76e045c36fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 23:04:14 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: scrapybot)\n",
      "2022-04-10 23:04:14 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 22.2.0, Python 3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:20:46) - [GCC 9.4.0], pyOpenSSL 22.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 36.0.1, Platform Linux-5.4.170+-x86_64-with-glibc2.31\n",
      "2022-04-10 23:04:14 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_LEVEL': 20,\n",
      " 'USER_AGENT': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:98.0) '\n",
      "               'Gecko/20100101 Firefox/98.0'}\n",
      "2022-04-10 23:04:14 [scrapy.extensions.telnet] INFO: Telnet Password: 91bdb6d506c5d925\n",
      "2022-04-10 23:04:14 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-04-10 23:04:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-04-10 23:04:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-04-10 23:04:14 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-04-10 23:04:14 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-04-10 23:04:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-04-10 23:04:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mont Saint Michel 0 Le Havre 0 Amiens 0 Probleme avec le score de Le Saint Vincent - 4/6P - 300m de la plage à Le Havre\n",
      "Bayeux 0 Probleme avec le score de Bonvallet - Studio paisible et chaleureux avec balcon à Amiens\n",
      "St Malo 0 Lille 0 Rouen 0 Paris 0 Strasbourg 0 Chateau du Haut Koenigsbourg 0 Eguisheim 0 Colmar 0 Besancon 0 Grenoble 0 Dijon 0 Annecy 0 La Rochelle 0 Mont Saint Michel 1 Paris 1 Strasbourg 1 Annecy 1 La Rochelle 1 Probleme avec le score de COSY FLAT MARKET à La Rochelle\n",
      "Mont Saint Michel 2 Paris 2 Strasbourg 2 Annecy 2 Probleme avec le score de Les Mouettes du Lac n2 - Grand Studio à 100 m du Lac d'Annecy à Annecy\n",
      "La Rochelle 2 Probleme avec le score de Studio La Rochelle, 2 pièces, 3 personnes - FR-1-246-210 à La Rochelle\n",
      "Probleme avec le score de Studio La Rochelle, 1 pièce, 2 personnes - FR-1-246-265 à La Rochelle\n",
      "Probleme avec le score de PLEIN CHARME AUX MINIMES CALME + PARKING + WIFI à La Rochelle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 23:04:37 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force \n",
      "2022-04-10 23:04:37 [scrapy.core.engine] INFO: Closing spider (shutdown)\n",
      "2022-04-10 23:04:41 [scrapy.extensions.feedexport] INFO: Stored json feed (74 items) in: booking_results.json\n",
      "2022-04-10 23:04:41 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 167036,\n",
      " 'downloader/request_count': 103,\n",
      " 'downloader/request_method_count/GET': 103,\n",
      " 'downloader/response_bytes': 19165990,\n",
      " 'downloader/response_count': 103,\n",
      " 'downloader/response_status_count/200': 102,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 27.058711,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'shutdown',\n",
      " 'finish_time': datetime.datetime(2022, 4, 10, 23, 4, 41, 908647),\n",
      " 'httpcompression/response_bytes': 131450905,\n",
      " 'httpcompression/response_count': 102,\n",
      " 'item_scraped_count': 74,\n",
      " 'log_count/INFO': 12,\n",
      " 'memusage/max': 133009408,\n",
      " 'memusage/startup': 133009408,\n",
      " 'request_depth_max': 4,\n",
      " 'response_received_count': 102,\n",
      " 'scheduler/dequeued': 103,\n",
      " 'scheduler/dequeued/memory': 103,\n",
      " 'scheduler/enqueued': 735,\n",
      " 'scheduler/enqueued/memory': 735,\n",
      " 'start_time': datetime.datetime(2022, 4, 10, 23, 4, 14, 849936)}\n",
      "2022-04-10 23:04:41 [scrapy.core.engine] INFO: Spider closed (shutdown)\n",
      "2022-04-10 23:04:41 [scrapy.core.engine] INFO: Error while scheduling new request\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/twisted/internet/task.py\", line 526, in _oneWorkUnit\n",
      "    result = next(self._iterator)\n",
      "StopIteration\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/twisted/internet/defer.py\", line 857, in _runCallbacks\n",
      "    current.result = callback(  # type: ignore[misc]\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/scrapy/core/engine.py\", line 187, in <lambda>\n",
      "    d.addBoth(lambda _: self.slot.nextcall.schedule())\n",
      "AttributeError: 'NoneType' object has no attribute 'nextcall'\n"
     ]
    }
   ],
   "source": [
    "filename = \"booking_results.json\"\n",
    "\n",
    "if filename in os.listdir('.'):\n",
    "    os.remove(filename)\n",
    "    \n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:98.0) Gecko/20100101 Firefox/98.0',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        filename : {\"format\": \"json\"},\n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(BookingSpiderTest)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b827a74d-23fa-4b86-8f59-203080232662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dash_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb22055b25f8ec49a6cc6c7b7bc59a6c9693cd138cc5513dd6a7d62b53572760"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
