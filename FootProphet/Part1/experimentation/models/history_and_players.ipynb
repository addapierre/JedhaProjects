{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpadda\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/padda/foot_LSTM_players_noFT/runs/2rceh8l8\" target=\"_blank\">glad-snow-1</a></strong> to <a href=\"https://wandb.ai/padda/foot_LSTM_players_noFT\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/padda/foot_LSTM_players_noFT/runs/2rceh8l8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x173e3da90>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.init(project=\"foot_LSTM_players_noFT\", entity=\"padda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm_metrics(true_label,pred_label, labels_):\n",
    "  cm = confusion_matrix(true_label,pred_label,labels = labels_)\n",
    "  cmDisp = ConfusionMatrixDisplay(cm,display_labels=labels_)\n",
    "  recall = { i:cm[i][i]/cm[i].sum()  for i in range(len(cm))}\n",
    "  precision = { i:cm[i][i]/cm[:,i].sum()  for i in range(len(cm))}\n",
    "  F1_score = { i: 2 / (1/recall[i] + 1/precision[i]) for i in range(len(cm))}\n",
    "  accuracy = (cm[0][0]+cm[1][1]+cm[2][2])/cm.sum()\n",
    "  print('accuracy: ', accuracy)\n",
    "  print('Recall: \\n',recall,'\\nPrecision: \\n', precision,'\\nF1-score: \\n',F1_score)\n",
    "  cmDisp.plot()\n",
    "  plt.show()\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_history = pd.read_csv('../data/train_LSTM/home_history.csv', index_col = 0,\\\n",
    "    dtype={'home_id' : np.int32, 'game_id' : np.int32, 'H_goals' : np.int8, 'H_goals_conceded' : np.int8,\\\n",
    "         'H_HorA' : np.int8, 'H_yellow_cards' : np.int8, 'H_red_cards' : np.int8}\\\n",
    "    )\n",
    "\n",
    "away_history = pd.read_csv('../data/train_LSTM/away_history.csv', index_col = 0,\\\n",
    "    dtype={'away_id' : np.int32, 'game_id' : np.int32, 'A_goals' : np.int8, 'A_goals_conceded' : np.int8,\\\n",
    "         'A_HorA' : np.int8, 'A_yellow_cards' : np.int8, 'A_red_cards' : np.int8}\\\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train_LSTM/train_LSTM.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['home_history','away_history'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# on rajoute des 0 et des 1 au début de chaque ligne pour indiquer si il y a un match ou pas\n",
    "# on inverse le game_history avec np.flip(game_history,axis=1) pour classer les matches du plus ancien au plus récent\n",
    "# Cela dit l'expérience montre que cette inversion n'a pas énormément d'impact sur les performances du modèle... \n",
    "\n",
    "def make_home_train(game_id):\n",
    "    game_history = home_history.loc[home_history.game_id == game_id,:].drop(columns=['home_id','game_id'])\n",
    "    if (n_lines := game_history.shape[0]) == 0:\n",
    "        return np.zeros((10,6)).astype(np.int8)\n",
    "    else:\n",
    "        return np.vstack(( np.zeros((10-n_lines,6)) , np.hstack(( np.ones((n_lines,1)) , np.flip(game_history.to_numpy(),axis=1))) )).astype(np.int8)\n",
    "\n",
    "def make_away_train(game_id):\n",
    "    game_history = away_history.loc[away_history.game_id == game_id,:].drop(columns=['away_id','game_id'])\n",
    "    if (n_lines := game_history.shape[0]) == 0:\n",
    "        return np.zeros((10,6)).astype(np.int8)\n",
    "    else:\n",
    "        return np.vstack(( np.zeros((10-n_lines,6)) , np.hstack(( np.ones((n_lines,1)) , np.flip(game_history.to_numpy(),axis=1))) )).astype(np.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29545/29545 [00:28<00:00, 1035.65it/s]\n",
      "100%|██████████| 29545/29545 [00:29<00:00, 1018.16it/s]\n"
     ]
    }
   ],
   "source": [
    "game_ids = train_df.game_id.astype(np.int32)\n",
    "\n",
    "train_home = game_ids.progress_apply(make_home_train)\n",
    "train_home = np.stack(train_home.values)\n",
    "\n",
    "train_away = game_ids.progress_apply(make_away_train)\n",
    "train_away = np.stack(train_away.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Home_result=train_df.Home_result.apply(lambda x: np.int8(0) if x>0 else (np.int8(2) if x<0 else np.int8(1)))\n",
    "y = train_df.Home_result.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    ". On divise (H/A)\\_(attack/defense/midfield) par 5  \n",
    ". idem pour (H/A)\\_(GK/ATK/DEF/MF)\\_(skill_moves/international_reputation/weak_foot) comme ce sont des notes sur 5  \n",
    ". tout le reste divisé par 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_GK</th>\n",
       "      <th>H_GK_overall</th>\n",
       "      <th>H_GK_potential</th>\n",
       "      <th>H_GK_skill_moves</th>\n",
       "      <th>H_GK_international_reputation</th>\n",
       "      <th>H_GK_weak_foot</th>\n",
       "      <th>H_GK_attacking_crossing</th>\n",
       "      <th>H_GK_attacking_heading_accuracy</th>\n",
       "      <th>H_GK_skill_curve</th>\n",
       "      <th>H_GK_skill_fk_accuracy</th>\n",
       "      <th>H_GK_movement_agility</th>\n",
       "      <th>H_GK_movement_reactions</th>\n",
       "      <th>H_GK_movement_balance</th>\n",
       "      <th>H_GK_power_shot_power</th>\n",
       "      <th>H_GK_power_jumping</th>\n",
       "      <th>H_GK_power_stamina</th>\n",
       "      <th>H_GK_mentality_aggression</th>\n",
       "      <th>H_GK_mentality_positioning</th>\n",
       "      <th>H_GK_mentality_penalties</th>\n",
       "      <th>H_GK_goalkeeping_diving</th>\n",
       "      <th>H_GK_goalkeeping_speed</th>\n",
       "      <th>H_attack</th>\n",
       "      <th>H_ATK_overall</th>\n",
       "      <th>H_ATK_potential</th>\n",
       "      <th>H_ATK_skill_moves</th>\n",
       "      <th>H_ATK_international_reputation</th>\n",
       "      <th>H_ATK_weak_foot</th>\n",
       "      <th>H_ATK_pace</th>\n",
       "      <th>H_ATK_shooting</th>\n",
       "      <th>H_ATK_passing</th>\n",
       "      <th>H_ATK_dribbling</th>\n",
       "      <th>H_ATK_defending</th>\n",
       "      <th>H_ATK_physic</th>\n",
       "      <th>H_ATK_attacking_crossing</th>\n",
       "      <th>H_ATK_attacking_heading_accuracy</th>\n",
       "      <th>H_ATK_skill_curve</th>\n",
       "      <th>H_ATK_skill_fk_accuracy</th>\n",
       "      <th>H_ATK_movement_agility</th>\n",
       "      <th>H_ATK_movement_reactions</th>\n",
       "      <th>H_ATK_movement_balance</th>\n",
       "      <th>H_ATK_power_shot_power</th>\n",
       "      <th>H_ATK_power_jumping</th>\n",
       "      <th>H_ATK_power_stamina</th>\n",
       "      <th>H_ATK_mentality_aggression</th>\n",
       "      <th>H_ATK_mentality_positioning</th>\n",
       "      <th>H_ATK_mentality_penalties</th>\n",
       "      <th>H_ATK_mentality_composure</th>\n",
       "      <th>H_defense</th>\n",
       "      <th>H_DEF_overall</th>\n",
       "      <th>H_DEF_potential</th>\n",
       "      <th>H_DEF_skill_moves</th>\n",
       "      <th>H_DEF_international_reputation</th>\n",
       "      <th>H_DEF_weak_foot</th>\n",
       "      <th>H_DEF_pace</th>\n",
       "      <th>H_DEF_shooting</th>\n",
       "      <th>H_DEF_passing</th>\n",
       "      <th>H_DEF_dribbling</th>\n",
       "      <th>H_DEF_defending</th>\n",
       "      <th>H_DEF_physic</th>\n",
       "      <th>H_DEF_attacking_crossing</th>\n",
       "      <th>H_DEF_attacking_heading_accuracy</th>\n",
       "      <th>H_DEF_skill_curve</th>\n",
       "      <th>H_DEF_skill_fk_accuracy</th>\n",
       "      <th>H_DEF_movement_agility</th>\n",
       "      <th>H_DEF_movement_reactions</th>\n",
       "      <th>H_DEF_movement_balance</th>\n",
       "      <th>H_DEF_power_shot_power</th>\n",
       "      <th>H_DEF_power_jumping</th>\n",
       "      <th>H_DEF_power_stamina</th>\n",
       "      <th>H_DEF_mentality_aggression</th>\n",
       "      <th>H_DEF_mentality_positioning</th>\n",
       "      <th>H_DEF_mentality_penalties</th>\n",
       "      <th>H_DEF_mentality_composure</th>\n",
       "      <th>H_midfield</th>\n",
       "      <th>H_MF_overall</th>\n",
       "      <th>H_MF_potential</th>\n",
       "      <th>H_MF_skill_moves</th>\n",
       "      <th>H_MF_international_reputation</th>\n",
       "      <th>H_MF_weak_foot</th>\n",
       "      <th>H_MF_pace</th>\n",
       "      <th>H_MF_shooting</th>\n",
       "      <th>H_MF_passing</th>\n",
       "      <th>H_MF_dribbling</th>\n",
       "      <th>H_MF_defending</th>\n",
       "      <th>H_MF_physic</th>\n",
       "      <th>H_MF_attacking_crossing</th>\n",
       "      <th>H_MF_attacking_heading_accuracy</th>\n",
       "      <th>H_MF_skill_curve</th>\n",
       "      <th>H_MF_skill_fk_accuracy</th>\n",
       "      <th>H_MF_movement_agility</th>\n",
       "      <th>H_MF_movement_reactions</th>\n",
       "      <th>H_MF_movement_balance</th>\n",
       "      <th>H_MF_power_shot_power</th>\n",
       "      <th>H_MF_power_jumping</th>\n",
       "      <th>H_MF_power_stamina</th>\n",
       "      <th>H_MF_mentality_aggression</th>\n",
       "      <th>H_MF_mentality_positioning</th>\n",
       "      <th>H_MF_mentality_penalties</th>\n",
       "      <th>H_MF_mentality_composure</th>\n",
       "      <th>A_GK</th>\n",
       "      <th>A_GK_overall</th>\n",
       "      <th>A_GK_potential</th>\n",
       "      <th>A_GK_skill_moves</th>\n",
       "      <th>A_GK_international_reputation</th>\n",
       "      <th>A_GK_weak_foot</th>\n",
       "      <th>A_GK_attacking_crossing</th>\n",
       "      <th>A_GK_attacking_heading_accuracy</th>\n",
       "      <th>A_GK_skill_curve</th>\n",
       "      <th>A_GK_skill_fk_accuracy</th>\n",
       "      <th>A_GK_movement_agility</th>\n",
       "      <th>A_GK_movement_reactions</th>\n",
       "      <th>A_GK_movement_balance</th>\n",
       "      <th>A_GK_power_shot_power</th>\n",
       "      <th>A_GK_power_jumping</th>\n",
       "      <th>A_GK_power_stamina</th>\n",
       "      <th>A_GK_mentality_aggression</th>\n",
       "      <th>A_GK_mentality_positioning</th>\n",
       "      <th>A_GK_mentality_penalties</th>\n",
       "      <th>A_GK_goalkeeping_diving</th>\n",
       "      <th>A_GK_goalkeeping_speed</th>\n",
       "      <th>A_attack</th>\n",
       "      <th>A_ATK_overall</th>\n",
       "      <th>A_ATK_potential</th>\n",
       "      <th>A_ATK_skill_moves</th>\n",
       "      <th>A_ATK_international_reputation</th>\n",
       "      <th>A_ATK_weak_foot</th>\n",
       "      <th>A_ATK_pace</th>\n",
       "      <th>A_ATK_shooting</th>\n",
       "      <th>A_ATK_passing</th>\n",
       "      <th>A_ATK_dribbling</th>\n",
       "      <th>A_ATK_defending</th>\n",
       "      <th>A_ATK_physic</th>\n",
       "      <th>A_ATK_attacking_crossing</th>\n",
       "      <th>A_ATK_attacking_heading_accuracy</th>\n",
       "      <th>A_ATK_skill_curve</th>\n",
       "      <th>A_ATK_skill_fk_accuracy</th>\n",
       "      <th>A_ATK_movement_agility</th>\n",
       "      <th>A_ATK_movement_reactions</th>\n",
       "      <th>A_ATK_movement_balance</th>\n",
       "      <th>A_ATK_power_shot_power</th>\n",
       "      <th>A_ATK_power_jumping</th>\n",
       "      <th>A_ATK_power_stamina</th>\n",
       "      <th>A_ATK_mentality_aggression</th>\n",
       "      <th>A_ATK_mentality_positioning</th>\n",
       "      <th>A_ATK_mentality_penalties</th>\n",
       "      <th>A_ATK_mentality_composure</th>\n",
       "      <th>A_defense</th>\n",
       "      <th>A_DEF_overall</th>\n",
       "      <th>A_DEF_potential</th>\n",
       "      <th>A_DEF_skill_moves</th>\n",
       "      <th>A_DEF_international_reputation</th>\n",
       "      <th>A_DEF_weak_foot</th>\n",
       "      <th>A_DEF_pace</th>\n",
       "      <th>A_DEF_shooting</th>\n",
       "      <th>A_DEF_passing</th>\n",
       "      <th>A_DEF_dribbling</th>\n",
       "      <th>A_DEF_defending</th>\n",
       "      <th>A_DEF_physic</th>\n",
       "      <th>A_DEF_attacking_crossing</th>\n",
       "      <th>A_DEF_attacking_heading_accuracy</th>\n",
       "      <th>A_DEF_skill_curve</th>\n",
       "      <th>A_DEF_skill_fk_accuracy</th>\n",
       "      <th>A_DEF_movement_agility</th>\n",
       "      <th>A_DEF_movement_reactions</th>\n",
       "      <th>A_DEF_movement_balance</th>\n",
       "      <th>A_DEF_power_shot_power</th>\n",
       "      <th>A_DEF_power_jumping</th>\n",
       "      <th>A_DEF_power_stamina</th>\n",
       "      <th>A_DEF_mentality_aggression</th>\n",
       "      <th>A_DEF_mentality_positioning</th>\n",
       "      <th>A_DEF_mentality_penalties</th>\n",
       "      <th>A_DEF_mentality_composure</th>\n",
       "      <th>A_midfield</th>\n",
       "      <th>A_MF_overall</th>\n",
       "      <th>A_MF_potential</th>\n",
       "      <th>A_MF_skill_moves</th>\n",
       "      <th>A_MF_international_reputation</th>\n",
       "      <th>A_MF_weak_foot</th>\n",
       "      <th>A_MF_pace</th>\n",
       "      <th>A_MF_shooting</th>\n",
       "      <th>A_MF_passing</th>\n",
       "      <th>A_MF_dribbling</th>\n",
       "      <th>A_MF_defending</th>\n",
       "      <th>A_MF_physic</th>\n",
       "      <th>A_MF_attacking_crossing</th>\n",
       "      <th>A_MF_attacking_heading_accuracy</th>\n",
       "      <th>A_MF_skill_curve</th>\n",
       "      <th>A_MF_skill_fk_accuracy</th>\n",
       "      <th>A_MF_movement_agility</th>\n",
       "      <th>A_MF_movement_reactions</th>\n",
       "      <th>A_MF_movement_balance</th>\n",
       "      <th>A_MF_power_shot_power</th>\n",
       "      <th>A_MF_power_jumping</th>\n",
       "      <th>A_MF_power_stamina</th>\n",
       "      <th>A_MF_mentality_aggression</th>\n",
       "      <th>A_MF_mentality_positioning</th>\n",
       "      <th>A_MF_mentality_penalties</th>\n",
       "      <th>A_MF_mentality_composure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>63.5625</td>\n",
       "      <td>73.3750</td>\n",
       "      <td>2.558594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.558594</td>\n",
       "      <td>72.7500</td>\n",
       "      <td>59.84375</td>\n",
       "      <td>57.25000</td>\n",
       "      <td>65.3125</td>\n",
       "      <td>26.984375</td>\n",
       "      <td>61.21875</td>\n",
       "      <td>61.03125</td>\n",
       "      <td>57.15625</td>\n",
       "      <td>53.90625</td>\n",
       "      <td>34.75000</td>\n",
       "      <td>64.2500</td>\n",
       "      <td>47.46875</td>\n",
       "      <td>60.75000</td>\n",
       "      <td>66.9375</td>\n",
       "      <td>53.18750</td>\n",
       "      <td>68.0625</td>\n",
       "      <td>64.87500</td>\n",
       "      <td>58.31250</td>\n",
       "      <td>62.15625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>65.7500</td>\n",
       "      <td>68.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>62.00000</td>\n",
       "      <td>51.50000</td>\n",
       "      <td>59.75000</td>\n",
       "      <td>63.00000</td>\n",
       "      <td>66.00000</td>\n",
       "      <td>68.0000</td>\n",
       "      <td>50.50000</td>\n",
       "      <td>61.7500</td>\n",
       "      <td>61.25000</td>\n",
       "      <td>52.00000</td>\n",
       "      <td>58.75000</td>\n",
       "      <td>64.00000</td>\n",
       "      <td>64.5000</td>\n",
       "      <td>62.50000</td>\n",
       "      <td>64.7500</td>\n",
       "      <td>68.5000</td>\n",
       "      <td>59.25000</td>\n",
       "      <td>53.50000</td>\n",
       "      <td>64.25000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.445312</td>\n",
       "      <td>69.1875</td>\n",
       "      <td>74.8125</td>\n",
       "      <td>2.941406</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.265625</td>\n",
       "      <td>71.4375</td>\n",
       "      <td>59.09375</td>\n",
       "      <td>65.3750</td>\n",
       "      <td>71.1250</td>\n",
       "      <td>49.1250</td>\n",
       "      <td>61.28125</td>\n",
       "      <td>57.75000</td>\n",
       "      <td>50.78125</td>\n",
       "      <td>54.43750</td>\n",
       "      <td>49.21875</td>\n",
       "      <td>73.4375</td>\n",
       "      <td>66.6250</td>\n",
       "      <td>71.9375</td>\n",
       "      <td>61.1875</td>\n",
       "      <td>62.90625</td>\n",
       "      <td>73.0000</td>\n",
       "      <td>52.09375</td>\n",
       "      <td>64.37500</td>\n",
       "      <td>56.34375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77.000</td>\n",
       "      <td>77.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>68.00000</td>\n",
       "      <td>79.00</td>\n",
       "      <td>58.0000</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>82.0000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>82.0000</td>\n",
       "      <td>58.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>82.4375</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.218750</td>\n",
       "      <td>75.8125</td>\n",
       "      <td>62.2500</td>\n",
       "      <td>65.31250</td>\n",
       "      <td>72.4375</td>\n",
       "      <td>32.90625</td>\n",
       "      <td>58.15625</td>\n",
       "      <td>63.40625</td>\n",
       "      <td>62.34375</td>\n",
       "      <td>67.5625</td>\n",
       "      <td>55.28125</td>\n",
       "      <td>75.3125</td>\n",
       "      <td>66.25000</td>\n",
       "      <td>66.6250</td>\n",
       "      <td>68.81250</td>\n",
       "      <td>59.2500</td>\n",
       "      <td>63.71875</td>\n",
       "      <td>45.53125</td>\n",
       "      <td>63.15625</td>\n",
       "      <td>55.96875</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.433594</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>2.224609</td>\n",
       "      <td>1.225586</td>\n",
       "      <td>3.126953</td>\n",
       "      <td>72.6250</td>\n",
       "      <td>43.78125</td>\n",
       "      <td>61.18750</td>\n",
       "      <td>67.43750</td>\n",
       "      <td>71.1875</td>\n",
       "      <td>70.4375</td>\n",
       "      <td>55.50000</td>\n",
       "      <td>72.5625</td>\n",
       "      <td>48.50000</td>\n",
       "      <td>31.37500</td>\n",
       "      <td>66.5000</td>\n",
       "      <td>70.1875</td>\n",
       "      <td>64.93750</td>\n",
       "      <td>61.84375</td>\n",
       "      <td>78.3125</td>\n",
       "      <td>73.0000</td>\n",
       "      <td>70.0625</td>\n",
       "      <td>55.31250</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.566406</td>\n",
       "      <td>75.1875</td>\n",
       "      <td>78.8125</td>\n",
       "      <td>2.611328</td>\n",
       "      <td>1.389648</td>\n",
       "      <td>3.609375</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>71.3750</td>\n",
       "      <td>71.5625</td>\n",
       "      <td>76.1250</td>\n",
       "      <td>54.81250</td>\n",
       "      <td>65.56250</td>\n",
       "      <td>62.34375</td>\n",
       "      <td>60.96875</td>\n",
       "      <td>68.93750</td>\n",
       "      <td>59.8125</td>\n",
       "      <td>75.3125</td>\n",
       "      <td>75.25000</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>73.5625</td>\n",
       "      <td>63.3750</td>\n",
       "      <td>78.8125</td>\n",
       "      <td>59.28125</td>\n",
       "      <td>74.68750</td>\n",
       "      <td>67.12500</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>75.9375</td>\n",
       "      <td>77.6875</td>\n",
       "      <td>3.333984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.333984</td>\n",
       "      <td>78.0000</td>\n",
       "      <td>73.62500</td>\n",
       "      <td>62.93750</td>\n",
       "      <td>77.6875</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>65.62500</td>\n",
       "      <td>61.00000</td>\n",
       "      <td>70.93750</td>\n",
       "      <td>55.03125</td>\n",
       "      <td>45.09375</td>\n",
       "      <td>79.3750</td>\n",
       "      <td>75.56250</td>\n",
       "      <td>70.62500</td>\n",
       "      <td>70.3125</td>\n",
       "      <td>72.43750</td>\n",
       "      <td>65.9375</td>\n",
       "      <td>53.62500</td>\n",
       "      <td>76.93750</td>\n",
       "      <td>64.93750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.187500</td>\n",
       "      <td>75.6875</td>\n",
       "      <td>76.5000</td>\n",
       "      <td>2.521484</td>\n",
       "      <td>1.477539</td>\n",
       "      <td>3.046875</td>\n",
       "      <td>66.87500</td>\n",
       "      <td>53.34375</td>\n",
       "      <td>70.18750</td>\n",
       "      <td>67.93750</td>\n",
       "      <td>75.68750</td>\n",
       "      <td>72.8125</td>\n",
       "      <td>71.31250</td>\n",
       "      <td>66.5625</td>\n",
       "      <td>60.25000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>66.00000</td>\n",
       "      <td>73.00000</td>\n",
       "      <td>66.2500</td>\n",
       "      <td>66.37500</td>\n",
       "      <td>66.6250</td>\n",
       "      <td>72.7500</td>\n",
       "      <td>73.93750</td>\n",
       "      <td>51.46875</td>\n",
       "      <td>51.09375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.822266</td>\n",
       "      <td>74.9375</td>\n",
       "      <td>80.6875</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.001953</td>\n",
       "      <td>71.9375</td>\n",
       "      <td>58.84375</td>\n",
       "      <td>68.8125</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>76.4375</td>\n",
       "      <td>76.37500</td>\n",
       "      <td>60.90625</td>\n",
       "      <td>65.31250</td>\n",
       "      <td>59.15625</td>\n",
       "      <td>56.12500</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>72.5625</td>\n",
       "      <td>70.1875</td>\n",
       "      <td>64.2500</td>\n",
       "      <td>72.18750</td>\n",
       "      <td>75.5000</td>\n",
       "      <td>77.87500</td>\n",
       "      <td>47.90625</td>\n",
       "      <td>55.03125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.125</td>\n",
       "      <td>72.8125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.18457</td>\n",
       "      <td>2.568359</td>\n",
       "      <td>13.9375</td>\n",
       "      <td>14.117188</td>\n",
       "      <td>14.734375</td>\n",
       "      <td>13.835938</td>\n",
       "      <td>41.15625</td>\n",
       "      <td>62.25</td>\n",
       "      <td>40.9375</td>\n",
       "      <td>48.71875</td>\n",
       "      <td>57.6875</td>\n",
       "      <td>30.296875</td>\n",
       "      <td>26.21875</td>\n",
       "      <td>10.523438</td>\n",
       "      <td>19.640625</td>\n",
       "      <td>68.6875</td>\n",
       "      <td>38.59375</td>\n",
       "      <td>2.066406</td>\n",
       "      <td>79.7500</td>\n",
       "      <td>79.8125</td>\n",
       "      <td>3.970703</td>\n",
       "      <td>1.967773</td>\n",
       "      <td>2.515625</td>\n",
       "      <td>80.9375</td>\n",
       "      <td>76.8125</td>\n",
       "      <td>70.12500</td>\n",
       "      <td>78.9375</td>\n",
       "      <td>35.15625</td>\n",
       "      <td>76.81250</td>\n",
       "      <td>63.46875</td>\n",
       "      <td>74.06250</td>\n",
       "      <td>63.6250</td>\n",
       "      <td>60.06250</td>\n",
       "      <td>78.4375</td>\n",
       "      <td>78.00000</td>\n",
       "      <td>66.8125</td>\n",
       "      <td>78.25000</td>\n",
       "      <td>75.0625</td>\n",
       "      <td>79.50000</td>\n",
       "      <td>67.18750</td>\n",
       "      <td>78.50000</td>\n",
       "      <td>70.12500</td>\n",
       "      <td>77.8125</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>78.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.666016</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>44.65625</td>\n",
       "      <td>61.34375</td>\n",
       "      <td>61.65625</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>76.0000</td>\n",
       "      <td>62.65625</td>\n",
       "      <td>72.6875</td>\n",
       "      <td>37.34375</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>62.0000</td>\n",
       "      <td>72.3125</td>\n",
       "      <td>61.34375</td>\n",
       "      <td>60.34375</td>\n",
       "      <td>73.3125</td>\n",
       "      <td>74.3125</td>\n",
       "      <td>73.3125</td>\n",
       "      <td>50.34375</td>\n",
       "      <td>50.34375</td>\n",
       "      <td>45.0</td>\n",
       "      <td>4.933594</td>\n",
       "      <td>73.6250</td>\n",
       "      <td>76.1875</td>\n",
       "      <td>3.199219</td>\n",
       "      <td>1.199219</td>\n",
       "      <td>3.199219</td>\n",
       "      <td>68.3750</td>\n",
       "      <td>64.6250</td>\n",
       "      <td>72.6875</td>\n",
       "      <td>72.9375</td>\n",
       "      <td>54.84375</td>\n",
       "      <td>62.34375</td>\n",
       "      <td>69.31250</td>\n",
       "      <td>54.81250</td>\n",
       "      <td>63.09375</td>\n",
       "      <td>66.5625</td>\n",
       "      <td>71.5000</td>\n",
       "      <td>70.31250</td>\n",
       "      <td>68.1875</td>\n",
       "      <td>70.0625</td>\n",
       "      <td>59.6875</td>\n",
       "      <td>68.0625</td>\n",
       "      <td>60.46875</td>\n",
       "      <td>58.78125</td>\n",
       "      <td>60.68750</td>\n",
       "      <td>58.65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.544922</td>\n",
       "      <td>77.3125</td>\n",
       "      <td>81.9375</td>\n",
       "      <td>3.029297</td>\n",
       "      <td>2.324219</td>\n",
       "      <td>2.970703</td>\n",
       "      <td>75.2500</td>\n",
       "      <td>74.62500</td>\n",
       "      <td>65.87500</td>\n",
       "      <td>73.5000</td>\n",
       "      <td>35.843750</td>\n",
       "      <td>72.25000</td>\n",
       "      <td>59.15625</td>\n",
       "      <td>72.06250</td>\n",
       "      <td>62.90625</td>\n",
       "      <td>58.93750</td>\n",
       "      <td>69.4375</td>\n",
       "      <td>75.50000</td>\n",
       "      <td>60.75000</td>\n",
       "      <td>78.7500</td>\n",
       "      <td>72.43750</td>\n",
       "      <td>73.1250</td>\n",
       "      <td>63.50000</td>\n",
       "      <td>78.87500</td>\n",
       "      <td>71.37500</td>\n",
       "      <td>2.128906</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>76.8750</td>\n",
       "      <td>80.8750</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>76.12500</td>\n",
       "      <td>54.25000</td>\n",
       "      <td>69.12500</td>\n",
       "      <td>72.50000</td>\n",
       "      <td>76.62500</td>\n",
       "      <td>73.3750</td>\n",
       "      <td>73.37500</td>\n",
       "      <td>69.2500</td>\n",
       "      <td>60.62500</td>\n",
       "      <td>53.37500</td>\n",
       "      <td>72.12500</td>\n",
       "      <td>74.25000</td>\n",
       "      <td>68.3750</td>\n",
       "      <td>64.50000</td>\n",
       "      <td>74.1250</td>\n",
       "      <td>79.8750</td>\n",
       "      <td>77.87500</td>\n",
       "      <td>57.00000</td>\n",
       "      <td>59.12500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.457031</td>\n",
       "      <td>81.1250</td>\n",
       "      <td>83.3125</td>\n",
       "      <td>3.230469</td>\n",
       "      <td>2.78125</td>\n",
       "      <td>3.683594</td>\n",
       "      <td>68.1875</td>\n",
       "      <td>73.18750</td>\n",
       "      <td>79.9375</td>\n",
       "      <td>81.8750</td>\n",
       "      <td>58.3750</td>\n",
       "      <td>70.75000</td>\n",
       "      <td>76.50000</td>\n",
       "      <td>57.65625</td>\n",
       "      <td>75.62500</td>\n",
       "      <td>72.00000</td>\n",
       "      <td>77.0625</td>\n",
       "      <td>80.4375</td>\n",
       "      <td>83.9375</td>\n",
       "      <td>77.5000</td>\n",
       "      <td>65.81250</td>\n",
       "      <td>77.3125</td>\n",
       "      <td>71.93750</td>\n",
       "      <td>78.31250</td>\n",
       "      <td>74.12500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.000</td>\n",
       "      <td>79.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>59.00000</td>\n",
       "      <td>74.00</td>\n",
       "      <td>55.0000</td>\n",
       "      <td>22.00000</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>79.0000</td>\n",
       "      <td>53.00000</td>\n",
       "      <td>1.721680</td>\n",
       "      <td>81.4375</td>\n",
       "      <td>83.3750</td>\n",
       "      <td>3.615234</td>\n",
       "      <td>2.386719</td>\n",
       "      <td>3.968750</td>\n",
       "      <td>75.6250</td>\n",
       "      <td>82.3125</td>\n",
       "      <td>73.68750</td>\n",
       "      <td>81.0000</td>\n",
       "      <td>37.03125</td>\n",
       "      <td>70.06250</td>\n",
       "      <td>68.50000</td>\n",
       "      <td>76.62500</td>\n",
       "      <td>78.0625</td>\n",
       "      <td>65.12500</td>\n",
       "      <td>73.5625</td>\n",
       "      <td>81.56250</td>\n",
       "      <td>61.3750</td>\n",
       "      <td>84.56250</td>\n",
       "      <td>70.1250</td>\n",
       "      <td>74.06250</td>\n",
       "      <td>50.21875</td>\n",
       "      <td>83.50000</td>\n",
       "      <td>80.06250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.945312</td>\n",
       "      <td>76.8750</td>\n",
       "      <td>79.5625</td>\n",
       "      <td>2.380859</td>\n",
       "      <td>2.189453</td>\n",
       "      <td>3.201172</td>\n",
       "      <td>77.8750</td>\n",
       "      <td>48.46875</td>\n",
       "      <td>65.62500</td>\n",
       "      <td>65.31250</td>\n",
       "      <td>67.8125</td>\n",
       "      <td>70.1875</td>\n",
       "      <td>67.50000</td>\n",
       "      <td>69.6875</td>\n",
       "      <td>55.90625</td>\n",
       "      <td>42.25000</td>\n",
       "      <td>69.1875</td>\n",
       "      <td>76.5625</td>\n",
       "      <td>66.25000</td>\n",
       "      <td>63.03125</td>\n",
       "      <td>76.6250</td>\n",
       "      <td>75.5000</td>\n",
       "      <td>69.3750</td>\n",
       "      <td>56.65625</td>\n",
       "      <td>48.18750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.332031</td>\n",
       "      <td>81.6875</td>\n",
       "      <td>84.1875</td>\n",
       "      <td>2.900391</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>3.001953</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>71.7500</td>\n",
       "      <td>77.1250</td>\n",
       "      <td>78.8750</td>\n",
       "      <td>65.81250</td>\n",
       "      <td>74.68750</td>\n",
       "      <td>69.37500</td>\n",
       "      <td>68.37500</td>\n",
       "      <td>70.75000</td>\n",
       "      <td>67.9375</td>\n",
       "      <td>78.1875</td>\n",
       "      <td>81.06250</td>\n",
       "      <td>75.9375</td>\n",
       "      <td>78.9375</td>\n",
       "      <td>70.3125</td>\n",
       "      <td>81.6875</td>\n",
       "      <td>71.62500</td>\n",
       "      <td>73.87500</td>\n",
       "      <td>70.68750</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.210938</td>\n",
       "      <td>66.0625</td>\n",
       "      <td>73.2500</td>\n",
       "      <td>3.523438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.761719</td>\n",
       "      <td>87.5625</td>\n",
       "      <td>53.40625</td>\n",
       "      <td>54.59375</td>\n",
       "      <td>69.6250</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>51.68750</td>\n",
       "      <td>69.75000</td>\n",
       "      <td>42.78125</td>\n",
       "      <td>58.84375</td>\n",
       "      <td>33.68750</td>\n",
       "      <td>83.3125</td>\n",
       "      <td>60.81250</td>\n",
       "      <td>69.56250</td>\n",
       "      <td>56.0000</td>\n",
       "      <td>57.09375</td>\n",
       "      <td>69.7500</td>\n",
       "      <td>41.21875</td>\n",
       "      <td>58.46875</td>\n",
       "      <td>47.46875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>65.7500</td>\n",
       "      <td>67.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.080078</td>\n",
       "      <td>60.65625</td>\n",
       "      <td>49.46875</td>\n",
       "      <td>57.56250</td>\n",
       "      <td>60.96875</td>\n",
       "      <td>66.18750</td>\n",
       "      <td>67.5000</td>\n",
       "      <td>46.78125</td>\n",
       "      <td>62.2500</td>\n",
       "      <td>63.28125</td>\n",
       "      <td>59.46875</td>\n",
       "      <td>56.03125</td>\n",
       "      <td>63.00000</td>\n",
       "      <td>64.6875</td>\n",
       "      <td>63.50000</td>\n",
       "      <td>62.5625</td>\n",
       "      <td>66.8125</td>\n",
       "      <td>61.28125</td>\n",
       "      <td>49.09375</td>\n",
       "      <td>60.53125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.789062</td>\n",
       "      <td>68.1250</td>\n",
       "      <td>74.0000</td>\n",
       "      <td>3.015625</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.373047</td>\n",
       "      <td>72.0000</td>\n",
       "      <td>59.15625</td>\n",
       "      <td>64.6250</td>\n",
       "      <td>70.6250</td>\n",
       "      <td>44.5625</td>\n",
       "      <td>58.87500</td>\n",
       "      <td>57.21875</td>\n",
       "      <td>51.28125</td>\n",
       "      <td>57.09375</td>\n",
       "      <td>52.28125</td>\n",
       "      <td>73.6875</td>\n",
       "      <td>65.1250</td>\n",
       "      <td>70.6250</td>\n",
       "      <td>62.1250</td>\n",
       "      <td>58.84375</td>\n",
       "      <td>71.1250</td>\n",
       "      <td>50.50000</td>\n",
       "      <td>64.31250</td>\n",
       "      <td>57.46875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>75.000</td>\n",
       "      <td>77.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>72.00</td>\n",
       "      <td>35.0000</td>\n",
       "      <td>43.00000</td>\n",
       "      <td>55.0000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>73.0000</td>\n",
       "      <td>55.00000</td>\n",
       "      <td>0.655762</td>\n",
       "      <td>67.0000</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>70.0000</td>\n",
       "      <td>50.00000</td>\n",
       "      <td>55.0000</td>\n",
       "      <td>53.00000</td>\n",
       "      <td>74.00000</td>\n",
       "      <td>47.00000</td>\n",
       "      <td>76.00000</td>\n",
       "      <td>53.0000</td>\n",
       "      <td>47.00000</td>\n",
       "      <td>35.0000</td>\n",
       "      <td>55.00000</td>\n",
       "      <td>33.0000</td>\n",
       "      <td>83.00000</td>\n",
       "      <td>73.0000</td>\n",
       "      <td>66.00000</td>\n",
       "      <td>48.00000</td>\n",
       "      <td>65.00000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>55.0000</td>\n",
       "      <td>5.343750</td>\n",
       "      <td>66.5625</td>\n",
       "      <td>69.1875</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>3.183594</td>\n",
       "      <td>67.9375</td>\n",
       "      <td>43.34375</td>\n",
       "      <td>56.43750</td>\n",
       "      <td>59.50000</td>\n",
       "      <td>65.1250</td>\n",
       "      <td>71.2500</td>\n",
       "      <td>54.03125</td>\n",
       "      <td>61.6250</td>\n",
       "      <td>45.75000</td>\n",
       "      <td>40.90625</td>\n",
       "      <td>61.1875</td>\n",
       "      <td>63.4375</td>\n",
       "      <td>61.37500</td>\n",
       "      <td>52.12500</td>\n",
       "      <td>72.2500</td>\n",
       "      <td>71.9375</td>\n",
       "      <td>64.6250</td>\n",
       "      <td>49.62500</td>\n",
       "      <td>50.90625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>69.6875</td>\n",
       "      <td>74.0625</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.458984</td>\n",
       "      <td>68.6875</td>\n",
       "      <td>64.4375</td>\n",
       "      <td>69.1875</td>\n",
       "      <td>71.8750</td>\n",
       "      <td>49.37500</td>\n",
       "      <td>65.18750</td>\n",
       "      <td>65.81250</td>\n",
       "      <td>50.78125</td>\n",
       "      <td>70.25000</td>\n",
       "      <td>62.5000</td>\n",
       "      <td>71.7500</td>\n",
       "      <td>67.50000</td>\n",
       "      <td>71.0000</td>\n",
       "      <td>69.6875</td>\n",
       "      <td>67.3125</td>\n",
       "      <td>73.5625</td>\n",
       "      <td>59.78125</td>\n",
       "      <td>65.75000</td>\n",
       "      <td>59.78125</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>71.3750</td>\n",
       "      <td>2.806641</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.912109</td>\n",
       "      <td>76.7500</td>\n",
       "      <td>63.59375</td>\n",
       "      <td>55.00000</td>\n",
       "      <td>67.3750</td>\n",
       "      <td>31.218750</td>\n",
       "      <td>65.50000</td>\n",
       "      <td>44.12500</td>\n",
       "      <td>62.15625</td>\n",
       "      <td>57.62500</td>\n",
       "      <td>45.06250</td>\n",
       "      <td>70.6250</td>\n",
       "      <td>65.62500</td>\n",
       "      <td>59.03125</td>\n",
       "      <td>71.8125</td>\n",
       "      <td>70.43750</td>\n",
       "      <td>66.6875</td>\n",
       "      <td>55.18750</td>\n",
       "      <td>67.37500</td>\n",
       "      <td>61.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.832031</td>\n",
       "      <td>65.5625</td>\n",
       "      <td>72.5625</td>\n",
       "      <td>2.173828</td>\n",
       "      <td>1.207031</td>\n",
       "      <td>3.207031</td>\n",
       "      <td>67.87500</td>\n",
       "      <td>40.56250</td>\n",
       "      <td>59.71875</td>\n",
       "      <td>62.03125</td>\n",
       "      <td>61.96875</td>\n",
       "      <td>65.6875</td>\n",
       "      <td>62.56250</td>\n",
       "      <td>63.1250</td>\n",
       "      <td>48.90625</td>\n",
       "      <td>47.12500</td>\n",
       "      <td>65.43750</td>\n",
       "      <td>63.53125</td>\n",
       "      <td>65.8750</td>\n",
       "      <td>55.21875</td>\n",
       "      <td>68.3125</td>\n",
       "      <td>67.9375</td>\n",
       "      <td>65.12500</td>\n",
       "      <td>50.78125</td>\n",
       "      <td>51.50000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.166016</td>\n",
       "      <td>69.4375</td>\n",
       "      <td>74.6875</td>\n",
       "      <td>2.316406</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.316406</td>\n",
       "      <td>66.7500</td>\n",
       "      <td>62.96875</td>\n",
       "      <td>65.1875</td>\n",
       "      <td>67.3125</td>\n",
       "      <td>55.4375</td>\n",
       "      <td>71.68750</td>\n",
       "      <td>53.09375</td>\n",
       "      <td>62.75000</td>\n",
       "      <td>61.71875</td>\n",
       "      <td>55.25000</td>\n",
       "      <td>65.6875</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>64.8750</td>\n",
       "      <td>71.5625</td>\n",
       "      <td>71.43750</td>\n",
       "      <td>79.6250</td>\n",
       "      <td>73.12500</td>\n",
       "      <td>57.68750</td>\n",
       "      <td>62.68750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.000</td>\n",
       "      <td>66.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>52.00000</td>\n",
       "      <td>64.00</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>25.00000</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>37.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>67.0000</td>\n",
       "      <td>60.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>63.7500</td>\n",
       "      <td>69.5625</td>\n",
       "      <td>2.666016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.228516</td>\n",
       "      <td>74.4375</td>\n",
       "      <td>59.8125</td>\n",
       "      <td>52.03125</td>\n",
       "      <td>64.7500</td>\n",
       "      <td>35.78125</td>\n",
       "      <td>64.12500</td>\n",
       "      <td>55.71875</td>\n",
       "      <td>46.53125</td>\n",
       "      <td>51.2500</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>71.1250</td>\n",
       "      <td>56.09375</td>\n",
       "      <td>71.6875</td>\n",
       "      <td>60.28125</td>\n",
       "      <td>66.3750</td>\n",
       "      <td>62.96875</td>\n",
       "      <td>63.34375</td>\n",
       "      <td>62.87500</td>\n",
       "      <td>57.75000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>63.7500</td>\n",
       "      <td>69.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>71.2500</td>\n",
       "      <td>41.25000</td>\n",
       "      <td>51.25000</td>\n",
       "      <td>53.25000</td>\n",
       "      <td>62.2500</td>\n",
       "      <td>71.5000</td>\n",
       "      <td>47.00000</td>\n",
       "      <td>65.2500</td>\n",
       "      <td>47.50000</td>\n",
       "      <td>41.25000</td>\n",
       "      <td>59.7500</td>\n",
       "      <td>59.0000</td>\n",
       "      <td>62.75000</td>\n",
       "      <td>56.75000</td>\n",
       "      <td>73.0000</td>\n",
       "      <td>66.5000</td>\n",
       "      <td>70.2500</td>\n",
       "      <td>42.00000</td>\n",
       "      <td>44.75000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.011719</td>\n",
       "      <td>67.9375</td>\n",
       "      <td>74.9375</td>\n",
       "      <td>2.664062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.662109</td>\n",
       "      <td>69.6250</td>\n",
       "      <td>56.6250</td>\n",
       "      <td>66.9375</td>\n",
       "      <td>69.3125</td>\n",
       "      <td>58.93750</td>\n",
       "      <td>63.62500</td>\n",
       "      <td>60.21875</td>\n",
       "      <td>53.25000</td>\n",
       "      <td>61.00000</td>\n",
       "      <td>58.9375</td>\n",
       "      <td>76.3125</td>\n",
       "      <td>63.34375</td>\n",
       "      <td>70.9375</td>\n",
       "      <td>69.6250</td>\n",
       "      <td>56.6875</td>\n",
       "      <td>73.5625</td>\n",
       "      <td>58.31250</td>\n",
       "      <td>59.96875</td>\n",
       "      <td>51.96875</td>\n",
       "      <td>22.25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   H_GK  H_GK_overall  H_GK_potential  H_GK_skill_moves  \\\n",
       "0   1.0          70.0            70.0               1.0   \n",
       "1   1.0          73.0            77.0               1.0   \n",
       "2   1.0          80.0            83.0               1.0   \n",
       "3   1.0          70.0            70.0               1.0   \n",
       "4   1.0          71.0            75.0               1.0   \n",
       "\n",
       "   H_GK_international_reputation  H_GK_weak_foot  H_GK_attacking_crossing  \\\n",
       "0                            1.0             3.0                     25.0   \n",
       "1                            1.0             2.0                     25.0   \n",
       "2                            2.0             3.0                     25.0   \n",
       "3                            1.0             3.0                     25.0   \n",
       "4                            2.0             2.0                     25.0   \n",
       "\n",
       "   H_GK_attacking_heading_accuracy  H_GK_skill_curve  H_GK_skill_fk_accuracy  \\\n",
       "0                             25.0              25.0                    25.0   \n",
       "1                             25.0              25.0                    25.0   \n",
       "2                             25.0              25.0                    25.0   \n",
       "3                             25.0              25.0                    25.0   \n",
       "4                             25.0              25.0                    25.0   \n",
       "\n",
       "   H_GK_movement_agility  H_GK_movement_reactions  H_GK_movement_balance  \\\n",
       "0                   36.0                     65.0                   41.0   \n",
       "1                   35.0                     49.0                   46.0   \n",
       "2                   55.0                     76.0                   46.0   \n",
       "3                   36.0                     65.0                   41.0   \n",
       "4                   58.0                     68.0                   46.0   \n",
       "\n",
       "   H_GK_power_shot_power  H_GK_power_jumping  H_GK_power_stamina  \\\n",
       "0                   27.0                45.0                40.0   \n",
       "1                   25.0                56.0                38.0   \n",
       "2                   34.0                38.0                38.0   \n",
       "3                   27.0                45.0                40.0   \n",
       "4                   25.0                83.0                33.0   \n",
       "\n",
       "   H_GK_mentality_aggression  H_GK_mentality_positioning  \\\n",
       "0                       30.0                        25.0   \n",
       "1                       25.0                        25.0   \n",
       "2                       39.0                        25.0   \n",
       "3                       30.0                        25.0   \n",
       "4                       23.0                        25.0   \n",
       "\n",
       "   H_GK_mentality_penalties  H_GK_goalkeeping_diving  H_GK_goalkeeping_speed  \\\n",
       "0                      45.0                     68.0                    38.0   \n",
       "1                      22.0                     73.0                    47.0   \n",
       "2                      47.0                     83.0                    45.0   \n",
       "3                      45.0                     68.0                    38.0   \n",
       "4                      25.0                     72.0                    56.0   \n",
       "\n",
       "   H_attack  H_ATK_overall  H_ATK_potential  H_ATK_skill_moves  \\\n",
       "0  0.555664        63.5625          73.3750           2.558594   \n",
       "1  3.000000        75.9375          77.6875           3.333984   \n",
       "2  1.544922        77.3125          81.9375           3.029297   \n",
       "3  1.210938        66.0625          73.2500           3.523438   \n",
       "4  2.000000        66.0000          71.3750           2.806641   \n",
       "\n",
       "   H_ATK_international_reputation  H_ATK_weak_foot  H_ATK_pace  \\\n",
       "0                        1.000000         3.558594     72.7500   \n",
       "1                        1.000000         3.333984     78.0000   \n",
       "2                        2.324219         2.970703     75.2500   \n",
       "3                        1.000000         3.761719     87.5625   \n",
       "4                        1.000000         2.912109     76.7500   \n",
       "\n",
       "   H_ATK_shooting  H_ATK_passing  H_ATK_dribbling  H_ATK_defending  \\\n",
       "0        59.84375       57.25000          65.3125        26.984375   \n",
       "1        73.62500       62.93750          77.6875        34.000000   \n",
       "2        74.62500       65.87500          73.5000        35.843750   \n",
       "3        53.40625       54.59375          69.6250        26.250000   \n",
       "4        63.59375       55.00000          67.3750        31.218750   \n",
       "\n",
       "   H_ATK_physic  H_ATK_attacking_crossing  H_ATK_attacking_heading_accuracy  \\\n",
       "0      61.21875                  61.03125                          57.15625   \n",
       "1      65.62500                  61.00000                          70.93750   \n",
       "2      72.25000                  59.15625                          72.06250   \n",
       "3      51.68750                  69.75000                          42.78125   \n",
       "4      65.50000                  44.12500                          62.15625   \n",
       "\n",
       "   H_ATK_skill_curve  H_ATK_skill_fk_accuracy  H_ATK_movement_agility  \\\n",
       "0           53.90625                 34.75000                 64.2500   \n",
       "1           55.03125                 45.09375                 79.3750   \n",
       "2           62.90625                 58.93750                 69.4375   \n",
       "3           58.84375                 33.68750                 83.3125   \n",
       "4           57.62500                 45.06250                 70.6250   \n",
       "\n",
       "   H_ATK_movement_reactions  H_ATK_movement_balance  H_ATK_power_shot_power  \\\n",
       "0                  47.46875                60.75000                 66.9375   \n",
       "1                  75.56250                70.62500                 70.3125   \n",
       "2                  75.50000                60.75000                 78.7500   \n",
       "3                  60.81250                69.56250                 56.0000   \n",
       "4                  65.62500                59.03125                 71.8125   \n",
       "\n",
       "   H_ATK_power_jumping  H_ATK_power_stamina  H_ATK_mentality_aggression  \\\n",
       "0             53.18750              68.0625                    64.87500   \n",
       "1             72.43750              65.9375                    53.62500   \n",
       "2             72.43750              73.1250                    63.50000   \n",
       "3             57.09375              69.7500                    41.21875   \n",
       "4             70.43750              66.6875                    55.18750   \n",
       "\n",
       "   H_ATK_mentality_positioning  H_ATK_mentality_penalties  \\\n",
       "0                     58.31250                   62.15625   \n",
       "1                     76.93750                   64.93750   \n",
       "2                     78.87500                   71.37500   \n",
       "3                     58.46875                   47.46875   \n",
       "4                     67.37500                   61.00000   \n",
       "\n",
       "   H_ATK_mentality_composure  H_defense  H_DEF_overall  H_DEF_potential  \\\n",
       "0                   0.000000   4.000000        65.7500          68.0000   \n",
       "1                   0.000000   4.187500        75.6875          76.5000   \n",
       "2                   2.128906   4.000000        76.8750          80.8750   \n",
       "3                   0.000000   4.000000        65.7500          67.0000   \n",
       "4                   0.000000   4.832031        65.5625          72.5625   \n",
       "\n",
       "   H_DEF_skill_moves  H_DEF_international_reputation  H_DEF_weak_foot  \\\n",
       "0           2.000000                        1.000000         3.250000   \n",
       "1           2.521484                        1.477539         3.046875   \n",
       "2           2.500000                        2.125000         2.625000   \n",
       "3           2.000000                        1.000000         3.080078   \n",
       "4           2.173828                        1.207031         3.207031   \n",
       "\n",
       "   H_DEF_pace  H_DEF_shooting  H_DEF_passing  H_DEF_dribbling  \\\n",
       "0    62.00000        51.50000       59.75000         63.00000   \n",
       "1    66.87500        53.34375       70.18750         67.93750   \n",
       "2    76.12500        54.25000       69.12500         72.50000   \n",
       "3    60.65625        49.46875       57.56250         60.96875   \n",
       "4    67.87500        40.56250       59.71875         62.03125   \n",
       "\n",
       "   H_DEF_defending  H_DEF_physic  H_DEF_attacking_crossing  \\\n",
       "0         66.00000       68.0000                  50.50000   \n",
       "1         75.68750       72.8125                  71.31250   \n",
       "2         76.62500       73.3750                  73.37500   \n",
       "3         66.18750       67.5000                  46.78125   \n",
       "4         61.96875       65.6875                  62.56250   \n",
       "\n",
       "   H_DEF_attacking_heading_accuracy  H_DEF_skill_curve  \\\n",
       "0                           61.7500           61.25000   \n",
       "1                           66.5625           60.25000   \n",
       "2                           69.2500           60.62500   \n",
       "3                           62.2500           63.28125   \n",
       "4                           63.1250           48.90625   \n",
       "\n",
       "   H_DEF_skill_fk_accuracy  H_DEF_movement_agility  H_DEF_movement_reactions  \\\n",
       "0                 52.00000                58.75000                  64.00000   \n",
       "1                 60.00000                66.00000                  73.00000   \n",
       "2                 53.37500                72.12500                  74.25000   \n",
       "3                 59.46875                56.03125                  63.00000   \n",
       "4                 47.12500                65.43750                  63.53125   \n",
       "\n",
       "   H_DEF_movement_balance  H_DEF_power_shot_power  H_DEF_power_jumping  \\\n",
       "0                 64.5000                62.50000              64.7500   \n",
       "1                 66.2500                66.37500              66.6250   \n",
       "2                 68.3750                64.50000              74.1250   \n",
       "3                 64.6875                63.50000              62.5625   \n",
       "4                 65.8750                55.21875              68.3125   \n",
       "\n",
       "   H_DEF_power_stamina  H_DEF_mentality_aggression  \\\n",
       "0              68.5000                    59.25000   \n",
       "1              72.7500                    73.93750   \n",
       "2              79.8750                    77.87500   \n",
       "3              66.8125                    61.28125   \n",
       "4              67.9375                    65.12500   \n",
       "\n",
       "   H_DEF_mentality_positioning  H_DEF_mentality_penalties  \\\n",
       "0                     53.50000                   64.25000   \n",
       "1                     51.46875                   51.09375   \n",
       "2                     57.00000                   59.12500   \n",
       "3                     49.09375                   60.53125   \n",
       "4                     50.78125                   51.50000   \n",
       "\n",
       "   H_DEF_mentality_composure  H_midfield  H_MF_overall  H_MF_potential  \\\n",
       "0                        0.0    5.445312       69.1875         74.8125   \n",
       "1                        0.0    2.822266       74.9375         80.6875   \n",
       "2                        0.0    4.457031       81.1250         83.3125   \n",
       "3                        0.0    4.789062       68.1250         74.0000   \n",
       "4                        0.0    3.166016       69.4375         74.6875   \n",
       "\n",
       "   H_MF_skill_moves  H_MF_international_reputation  H_MF_weak_foot  H_MF_pace  \\\n",
       "0          2.941406                        1.00000        3.265625    71.4375   \n",
       "1          2.000000                        1.00000        3.001953    71.9375   \n",
       "2          3.230469                        2.78125        3.683594    68.1875   \n",
       "3          3.015625                        1.00000        3.373047    72.0000   \n",
       "4          2.316406                        1.00000        3.316406    66.7500   \n",
       "\n",
       "   H_MF_shooting  H_MF_passing  H_MF_dribbling  H_MF_defending  H_MF_physic  \\\n",
       "0       59.09375       65.3750         71.1250         49.1250     61.28125   \n",
       "1       58.84375       68.8125         69.0000         76.4375     76.37500   \n",
       "2       73.18750       79.9375         81.8750         58.3750     70.75000   \n",
       "3       59.15625       64.6250         70.6250         44.5625     58.87500   \n",
       "4       62.96875       65.1875         67.3125         55.4375     71.68750   \n",
       "\n",
       "   H_MF_attacking_crossing  H_MF_attacking_heading_accuracy  H_MF_skill_curve  \\\n",
       "0                 57.75000                         50.78125          54.43750   \n",
       "1                 60.90625                         65.31250          59.15625   \n",
       "2                 76.50000                         57.65625          75.62500   \n",
       "3                 57.21875                         51.28125          57.09375   \n",
       "4                 53.09375                         62.75000          61.71875   \n",
       "\n",
       "   H_MF_skill_fk_accuracy  H_MF_movement_agility  H_MF_movement_reactions  \\\n",
       "0                49.21875                73.4375                  66.6250   \n",
       "1                56.12500                69.0000                  72.5625   \n",
       "2                72.00000                77.0625                  80.4375   \n",
       "3                52.28125                73.6875                  65.1250   \n",
       "4                55.25000                65.6875                  69.0000   \n",
       "\n",
       "   H_MF_movement_balance  H_MF_power_shot_power  H_MF_power_jumping  \\\n",
       "0                71.9375                61.1875            62.90625   \n",
       "1                70.1875                64.2500            72.18750   \n",
       "2                83.9375                77.5000            65.81250   \n",
       "3                70.6250                62.1250            58.84375   \n",
       "4                64.8750                71.5625            71.43750   \n",
       "\n",
       "   H_MF_power_stamina  H_MF_mentality_aggression  H_MF_mentality_positioning  \\\n",
       "0             73.0000                   52.09375                    64.37500   \n",
       "1             75.5000                   77.87500                    47.90625   \n",
       "2             77.3125                   71.93750                    78.31250   \n",
       "3             71.1250                   50.50000                    64.31250   \n",
       "4             79.6250                   73.12500                    57.68750   \n",
       "\n",
       "   H_MF_mentality_penalties  H_MF_mentality_composure  A_GK  A_GK_overall  \\\n",
       "0                  56.34375                       0.0   1.0        77.000   \n",
       "1                  55.03125                       0.0   1.0        68.125   \n",
       "2                  74.12500                       0.0   1.0        79.000   \n",
       "3                  57.46875                       0.0   1.0        75.000   \n",
       "4                  62.68750                       0.0   1.0        63.000   \n",
       "\n",
       "   A_GK_potential  A_GK_skill_moves  A_GK_international_reputation  \\\n",
       "0         77.0000               1.0                        2.00000   \n",
       "1         72.8125               1.0                        1.18457   \n",
       "2         79.0000               1.0                        2.00000   \n",
       "3         77.0000               1.0                        1.00000   \n",
       "4         66.0000               1.0                        1.00000   \n",
       "\n",
       "   A_GK_weak_foot  A_GK_attacking_crossing  A_GK_attacking_heading_accuracy  \\\n",
       "0        3.000000                  25.0000                        25.000000   \n",
       "1        2.568359                  13.9375                        14.117188   \n",
       "2        3.000000                  25.0000                        21.000000   \n",
       "3        2.000000                  25.0000                        25.000000   \n",
       "4        3.000000                  25.0000                        25.000000   \n",
       "\n",
       "   A_GK_skill_curve  A_GK_skill_fk_accuracy  A_GK_movement_agility  \\\n",
       "0         25.000000               25.000000               68.00000   \n",
       "1         14.734375               13.835938               41.15625   \n",
       "2         22.000000               25.000000               59.00000   \n",
       "3         25.000000               25.000000               41.00000   \n",
       "4         25.000000               25.000000               52.00000   \n",
       "\n",
       "   A_GK_movement_reactions  A_GK_movement_balance  A_GK_power_shot_power  \\\n",
       "0                    79.00                58.0000               23.00000   \n",
       "1                    62.25                40.9375               48.71875   \n",
       "2                    74.00                55.0000               22.00000   \n",
       "3                    72.00                35.0000               43.00000   \n",
       "4                    64.00                43.0000               25.00000   \n",
       "\n",
       "   A_GK_power_jumping  A_GK_power_stamina  A_GK_mentality_aggression  \\\n",
       "0             82.0000           42.000000                   34.00000   \n",
       "1             57.6875           30.296875                   26.21875   \n",
       "2             70.0000           44.000000                   45.00000   \n",
       "3             55.0000           37.000000                   34.00000   \n",
       "4             51.0000           42.000000                   37.00000   \n",
       "\n",
       "   A_GK_mentality_positioning  A_GK_mentality_penalties  \\\n",
       "0                   25.000000                 29.000000   \n",
       "1                   10.523438                 19.640625   \n",
       "2                   25.000000                 25.000000   \n",
       "3                   25.000000                 23.000000   \n",
       "4                   25.000000                 25.000000   \n",
       "\n",
       "   A_GK_goalkeeping_diving  A_GK_goalkeeping_speed  A_attack  A_ATK_overall  \\\n",
       "0                  82.0000                58.00000  3.000000        71.0000   \n",
       "1                  68.6875                38.59375  2.066406        79.7500   \n",
       "2                  79.0000                53.00000  1.721680        81.4375   \n",
       "3                  73.0000                55.00000  0.655762        67.0000   \n",
       "4                  67.0000                60.00000  3.000000        63.7500   \n",
       "\n",
       "   A_ATK_potential  A_ATK_skill_moves  A_ATK_international_reputation  \\\n",
       "0          82.4375           3.000000                        1.000000   \n",
       "1          79.8125           3.970703                        1.967773   \n",
       "2          83.3750           3.615234                        2.386719   \n",
       "3          70.0000           2.000000                        1.000000   \n",
       "4          69.5625           2.666016                        1.000000   \n",
       "\n",
       "   A_ATK_weak_foot  A_ATK_pace  A_ATK_shooting  A_ATK_passing  \\\n",
       "0         3.218750     75.8125         62.2500       65.31250   \n",
       "1         2.515625     80.9375         76.8125       70.12500   \n",
       "2         3.968750     75.6250         82.3125       73.68750   \n",
       "3         2.000000     43.0000         70.0000       50.00000   \n",
       "4         3.228516     74.4375         59.8125       52.03125   \n",
       "\n",
       "   A_ATK_dribbling  A_ATK_defending  A_ATK_physic  A_ATK_attacking_crossing  \\\n",
       "0          72.4375         32.90625      58.15625                  63.40625   \n",
       "1          78.9375         35.15625      76.81250                  63.46875   \n",
       "2          81.0000         37.03125      70.06250                  68.50000   \n",
       "3          55.0000         53.00000      74.00000                  47.00000   \n",
       "4          64.7500         35.78125      64.12500                  55.71875   \n",
       "\n",
       "   A_ATK_attacking_heading_accuracy  A_ATK_skill_curve  \\\n",
       "0                          62.34375            67.5625   \n",
       "1                          74.06250            63.6250   \n",
       "2                          76.62500            78.0625   \n",
       "3                          76.00000            53.0000   \n",
       "4                          46.53125            51.2500   \n",
       "\n",
       "   A_ATK_skill_fk_accuracy  A_ATK_movement_agility  A_ATK_movement_reactions  \\\n",
       "0                 55.28125                 75.3125                  66.25000   \n",
       "1                 60.06250                 78.4375                  78.00000   \n",
       "2                 65.12500                 73.5625                  81.56250   \n",
       "3                 47.00000                 35.0000                  55.00000   \n",
       "4                 40.00000                 71.1250                  56.09375   \n",
       "\n",
       "   A_ATK_movement_balance  A_ATK_power_shot_power  A_ATK_power_jumping  \\\n",
       "0                 66.6250                68.81250              59.2500   \n",
       "1                 66.8125                78.25000              75.0625   \n",
       "2                 61.3750                84.56250              70.1250   \n",
       "3                 33.0000                83.00000              73.0000   \n",
       "4                 71.6875                60.28125              66.3750   \n",
       "\n",
       "   A_ATK_power_stamina  A_ATK_mentality_aggression  \\\n",
       "0             63.71875                    45.53125   \n",
       "1             79.50000                    67.18750   \n",
       "2             74.06250                    50.21875   \n",
       "3             66.00000                    48.00000   \n",
       "4             62.96875                    63.34375   \n",
       "\n",
       "   A_ATK_mentality_positioning  A_ATK_mentality_penalties  \\\n",
       "0                     63.15625                   55.96875   \n",
       "1                     78.50000                   70.12500   \n",
       "2                     83.50000                   80.06250   \n",
       "3                     65.00000                   60.00000   \n",
       "4                     62.87500                   57.75000   \n",
       "\n",
       "   A_ATK_mentality_composure  A_defense  A_DEF_overall  A_DEF_potential  \\\n",
       "0                     0.0000   4.433594        72.1250          76.0000   \n",
       "1                    77.8125   3.000000        76.0000          78.0000   \n",
       "2                     0.0000   4.945312        76.8750          79.5625   \n",
       "3                    55.0000   5.343750        66.5625          69.1875   \n",
       "4                     0.0000   4.000000        63.7500          69.0000   \n",
       "\n",
       "   A_DEF_skill_moves  A_DEF_international_reputation  A_DEF_weak_foot  \\\n",
       "0           2.224609                        1.225586         3.126953   \n",
       "1           2.000000                        2.000000         3.666016   \n",
       "2           2.380859                        2.189453         3.201172   \n",
       "3           2.000000                        1.187500         3.183594   \n",
       "4           2.000000                        1.000000         2.500000   \n",
       "\n",
       "   A_DEF_pace  A_DEF_shooting  A_DEF_passing  A_DEF_dribbling  \\\n",
       "0     72.6250        43.78125       61.18750         67.43750   \n",
       "1     74.0000        44.65625       61.34375         61.65625   \n",
       "2     77.8750        48.46875       65.62500         65.31250   \n",
       "3     67.9375        43.34375       56.43750         59.50000   \n",
       "4     71.2500        41.25000       51.25000         53.25000   \n",
       "\n",
       "   A_DEF_defending  A_DEF_physic  A_DEF_attacking_crossing  \\\n",
       "0          71.1875       70.4375                  55.50000   \n",
       "1          76.0000       76.0000                  62.65625   \n",
       "2          67.8125       70.1875                  67.50000   \n",
       "3          65.1250       71.2500                  54.03125   \n",
       "4          62.2500       71.5000                  47.00000   \n",
       "\n",
       "   A_DEF_attacking_heading_accuracy  A_DEF_skill_curve  \\\n",
       "0                           72.5625           48.50000   \n",
       "1                           72.6875           37.34375   \n",
       "2                           69.6875           55.90625   \n",
       "3                           61.6250           45.75000   \n",
       "4                           65.2500           47.50000   \n",
       "\n",
       "   A_DEF_skill_fk_accuracy  A_DEF_movement_agility  A_DEF_movement_reactions  \\\n",
       "0                 31.37500                 66.5000                   70.1875   \n",
       "1                 45.00000                 62.0000                   72.3125   \n",
       "2                 42.25000                 69.1875                   76.5625   \n",
       "3                 40.90625                 61.1875                   63.4375   \n",
       "4                 41.25000                 59.7500                   59.0000   \n",
       "\n",
       "   A_DEF_movement_balance  A_DEF_power_shot_power  A_DEF_power_jumping  \\\n",
       "0                64.93750                61.84375              78.3125   \n",
       "1                61.34375                60.34375              73.3125   \n",
       "2                66.25000                63.03125              76.6250   \n",
       "3                61.37500                52.12500              72.2500   \n",
       "4                62.75000                56.75000              73.0000   \n",
       "\n",
       "   A_DEF_power_stamina  A_DEF_mentality_aggression  \\\n",
       "0              73.0000                     70.0625   \n",
       "1              74.3125                     73.3125   \n",
       "2              75.5000                     69.3750   \n",
       "3              71.9375                     64.6250   \n",
       "4              66.5000                     70.2500   \n",
       "\n",
       "   A_DEF_mentality_positioning  A_DEF_mentality_penalties  \\\n",
       "0                     55.31250                   40.00000   \n",
       "1                     50.34375                   50.34375   \n",
       "2                     56.65625                   48.18750   \n",
       "3                     49.62500                   50.90625   \n",
       "4                     42.00000                   44.75000   \n",
       "\n",
       "   A_DEF_mentality_composure  A_midfield  A_MF_overall  A_MF_potential  \\\n",
       "0                        0.0    2.566406       75.1875         78.8125   \n",
       "1                       45.0    4.933594       73.6250         76.1875   \n",
       "2                        0.0    3.332031       81.6875         84.1875   \n",
       "3                        0.0    4.000000       69.6875         74.0625   \n",
       "4                        0.0    3.011719       67.9375         74.9375   \n",
       "\n",
       "   A_MF_skill_moves  A_MF_international_reputation  A_MF_weak_foot  A_MF_pace  \\\n",
       "0          2.611328                       1.389648        3.609375    73.5000   \n",
       "1          3.199219                       1.199219        3.199219    68.3750   \n",
       "2          2.900391                       2.750000        3.001953    74.0000   \n",
       "3          3.000000                       1.250000        3.458984    68.6875   \n",
       "4          2.664062                       1.000000        3.662109    69.6250   \n",
       "\n",
       "   A_MF_shooting  A_MF_passing  A_MF_dribbling  A_MF_defending  A_MF_physic  \\\n",
       "0        71.3750       71.5625         76.1250        54.81250     65.56250   \n",
       "1        64.6250       72.6875         72.9375        54.84375     62.34375   \n",
       "2        71.7500       77.1250         78.8750        65.81250     74.68750   \n",
       "3        64.4375       69.1875         71.8750        49.37500     65.18750   \n",
       "4        56.6250       66.9375         69.3125        58.93750     63.62500   \n",
       "\n",
       "   A_MF_attacking_crossing  A_MF_attacking_heading_accuracy  A_MF_skill_curve  \\\n",
       "0                 62.34375                         60.96875          68.93750   \n",
       "1                 69.31250                         54.81250          63.09375   \n",
       "2                 69.37500                         68.37500          70.75000   \n",
       "3                 65.81250                         50.78125          70.25000   \n",
       "4                 60.21875                         53.25000          61.00000   \n",
       "\n",
       "   A_MF_skill_fk_accuracy  A_MF_movement_agility  A_MF_movement_reactions  \\\n",
       "0                 59.8125                75.3125                 75.25000   \n",
       "1                 66.5625                71.5000                 70.31250   \n",
       "2                 67.9375                78.1875                 81.06250   \n",
       "3                 62.5000                71.7500                 67.50000   \n",
       "4                 58.9375                76.3125                 63.34375   \n",
       "\n",
       "   A_MF_movement_balance  A_MF_power_shot_power  A_MF_power_jumping  \\\n",
       "0                73.5000                73.5625             63.3750   \n",
       "1                68.1875                70.0625             59.6875   \n",
       "2                75.9375                78.9375             70.3125   \n",
       "3                71.0000                69.6875             67.3125   \n",
       "4                70.9375                69.6250             56.6875   \n",
       "\n",
       "   A_MF_power_stamina  A_MF_mentality_aggression  A_MF_mentality_positioning  \\\n",
       "0             78.8125                   59.28125                    74.68750   \n",
       "1             68.0625                   60.46875                    58.78125   \n",
       "2             81.6875                   71.62500                    73.87500   \n",
       "3             73.5625                   59.78125                    65.75000   \n",
       "4             73.5625                   58.31250                    59.96875   \n",
       "\n",
       "   A_MF_mentality_penalties  A_MF_mentality_composure  \n",
       "0                  67.12500                   0.00000  \n",
       "1                  60.68750                  58.65625  \n",
       "2                  70.68750                   0.00000  \n",
       "3                  59.78125                   0.00000  \n",
       "4                  51.96875                  22.25000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_df.loc[:,train_df.columns[1:-1]].astype(np.float16).copy()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_select = [x+y for x in ['H_','A_'] for y in ['attack', 'defense', 'midfield']]\n",
    "X.loc[:,col_select]=X.loc[:,col_select]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_select1 = [x+y+z for x in ['H_','A_'] for y in ['GK_','ATK_','DEF_','MF_'] for z in ['skill_moves','international_reputation','weak_foot']]\n",
    "X.loc[:,col_select1]=X.loc[:,col_select1]/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_select2 = col_select+col_select1+['H_GK','A_GK']\n",
    "col_mask = [x  for x in X.columns if (x not in col_select2)]\n",
    "X.loc[:,col_mask]=X.loc[:,col_mask]/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "  def __init__(self, xh_data, xa_data, xplayer_data, y_data, batch_size):\n",
    "    self.xh, self.xa, self.xplayer, self.y = xh_data, xa_data, xplayer_data,y_data\n",
    "    self.batch_size = batch_size\n",
    "    self.num_batches = np.ceil(len(xh_data) / batch_size)\n",
    "    self.batch_idx = np.array_split(range(len(xh_data)), self.num_batches)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.batch_idx)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    batch_xh = self.xh[self.batch_idx[idx]]\n",
    "    batch_xa = self.xa[self.batch_idx[idx]]\n",
    "    batch_xplayer = self.xplayer[self.batch_idx[idx]]\n",
    "    batch_y = self.y[self.batch_idx[idx]]\n",
    "    return [batch_xh,batch_xa,batch_xplayer], batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "XH_train, XH_test, XA_train, XA_test, Xplayers_train, Xplayers_test, y_train, y_test \\\n",
    "    = train_test_split(train_home, train_away, X, y, test_size = 0.25, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataGenerator(XH_train,XA_train,Xplayers_train,y_train,64)\n",
    "test_data = DataGenerator(XH_test,XA_test,Xplayers_test,y_test,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle \n",
    "#### avec pré-entrainement sans fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = tf.keras.models.load_model('../data/models/LSTM_only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_train_input  away_train_input  home_LSTM_2  away_LSTM_2  concatenate_23  "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't set the attribute \"kernel_regularizer\", likely because it conflicts with an existing read-only @property of the object. Please choose a different name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/foot_env/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=bad-super-call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fw/vk6b0tfx7kvg_95mz0b5mts80000gn/T/ipykernel_22125/124612915.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'  '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_LSTM_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kernel_regularizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1L2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/foot_env/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   2777\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=bad-super-call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2779\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   2780\u001b[0m             ('Can\\'t set the attribute \"{}\", likely because it conflicts with '\n\u001b[1;32m   2781\u001b[0m              \u001b[0;34m'an existing read-only @property of the object. Please choose a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't set the attribute \"kernel_regularizer\", likely because it conflicts with an existing read-only @property of the object. Please choose a different name."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectTimeout), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "# on retire la dernière couche du modèle\n",
    "penultimate_layer = LSTM_model.layers[-7]\n",
    "base_LSTM_model = tf.keras.Model(LSTM_model.inputs,penultimate_layer.output)\n",
    "base_LSTM_model.trainable=True\n",
    "for layer in base_LSTM_model.layers:\n",
    "    print(layer.name,end='  ')\n",
    "\n",
    "setattr(base_LSTM_model.layers[2], 'kernel_regularizer', L1L2(l1=0.01, l2=0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home_train_input  away_train_input  home_LSTM_2  away_LSTM_2  concatenate_23  dropout_59  "
     ]
    }
   ],
   "source": [
    "base_LSTM_model = LSTM_model\n",
    "base_LSTM_model.trainable=True\n",
    "for layer in base_LSTM_model.layers[:-5]:\n",
    "    print(layer.name,end='  ')\n",
    "    #layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_input = tf.keras.Input(shape = (10,6), name = 'home_train_input')  \n",
    "away_input = tf.keras.Input(shape = (10,6), name = 'away_train_input')  \n",
    "LSTM_output = base_LSTM_model([home_input,away_input])  \n",
    "LSTM_output = tf.keras.layers.Dropout(0.34)(LSTM_output)  \n",
    "#LSTM_output = tf.keras.layers.BatchNormalization()(LSTM_output)\n",
    "\n",
    "\n",
    "player_input = tf.keras.Input(shape = (198), name = 'player_train_input')  \n",
    "concat_layer = tf.keras.layers.Concatenate(name='concat_layer')([LSTM_output,player_input])  \n",
    "Dense1 = tf.keras.layers.Dense(256,'relu',name = 'Dense1')(concat_layer)  \n",
    "Dense1 = tf.keras.layers.Dropout(0.234375)(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dense(128,'relu',name='Dense2')(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dropout(0.125)(Dense2)  \n",
    "Dense_output = tf.keras.layers.Dense(3, 'softmax', name='output')(Dense2)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_input = tf.keras.Input(shape = (10,6), name = 'home_train_input')  \n",
    "away_input = tf.keras.Input(shape = (10,6), name = 'away_train_input')  \n",
    "LSTM_output = base_LSTM_model([home_input,away_input])  \n",
    "#LSTM_output = tf.keras.layers.Dropout(0.34)(LSTM_output)  \n",
    "#LSTM_output = tf.keras.layers.BatchNormalization()(LSTM_output)\n",
    "\n",
    "\n",
    "player_input = tf.keras.Input(shape = (198), name = 'player_train_input')    \n",
    "Dense1 = tf.keras.layers.Dense(256,'relu',name = 'Dense1')(player_input)  \n",
    "Dense1 = tf.keras.layers.Dropout(0.2)(Dense1)  \n",
    "\n",
    "concat_layer = tf.keras.layers.Concatenate(name='concat_layer')([LSTM_output,Dense1])  \n",
    "concat_layer = tf.keras.layers.Dropout(0.234375)(concat_layer)  \n",
    "Dense2 = tf.keras.layers.Dense(128,'relu',name='Dense2')(concat_layer)  \n",
    "Dense2 = tf.keras.layers.Dropout(0.125)(Dense2)  \n",
    "Dense_output = tf.keras.layers.Dense(3, 'softmax', name='output')(Dense2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_noFT = tf.keras.Model(inputs=[home_input,away_input,player_input], outputs=Dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "opt = Adam(0.0003)\n",
    "loss = SparseCategoricalCrossentropy()\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_noFT.compile(optimizer=opt, loss=loss, metrics = [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "347/347 [==============================] - 11s 13ms/step - loss: 1.0122 - sparse_categorical_accuracy: 0.5072 - val_loss: 1.0153 - val_sparse_categorical_accuracy: 0.5017\n",
      "Epoch 2/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9860 - sparse_categorical_accuracy: 0.5306 - val_loss: 1.0212 - val_sparse_categorical_accuracy: 0.5087\n",
      "Epoch 3/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9766 - sparse_categorical_accuracy: 0.5348 - val_loss: 1.0082 - val_sparse_categorical_accuracy: 0.5125\n",
      "Epoch 4/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9719 - sparse_categorical_accuracy: 0.5379 - val_loss: 1.0076 - val_sparse_categorical_accuracy: 0.5120\n",
      "Epoch 5/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9690 - sparse_categorical_accuracy: 0.5434 - val_loss: 1.0034 - val_sparse_categorical_accuracy: 0.5131\n",
      "Epoch 6/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9633 - sparse_categorical_accuracy: 0.5445 - val_loss: 1.0036 - val_sparse_categorical_accuracy: 0.5135\n",
      "Epoch 7/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9603 - sparse_categorical_accuracy: 0.5432 - val_loss: 1.0148 - val_sparse_categorical_accuracy: 0.5118\n",
      "Epoch 8/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9585 - sparse_categorical_accuracy: 0.5465 - val_loss: 1.0083 - val_sparse_categorical_accuracy: 0.5090\n",
      "Epoch 9/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9566 - sparse_categorical_accuracy: 0.5482 - val_loss: 1.0097 - val_sparse_categorical_accuracy: 0.5063\n",
      "Epoch 10/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9525 - sparse_categorical_accuracy: 0.5514 - val_loss: 1.0084 - val_sparse_categorical_accuracy: 0.5109\n",
      "Epoch 11/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9532 - sparse_categorical_accuracy: 0.5528 - val_loss: 1.0115 - val_sparse_categorical_accuracy: 0.5064\n",
      "Epoch 12/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9501 - sparse_categorical_accuracy: 0.5522 - val_loss: 1.0242 - val_sparse_categorical_accuracy: 0.4994\n",
      "Epoch 13/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9468 - sparse_categorical_accuracy: 0.5553 - val_loss: 1.0118 - val_sparse_categorical_accuracy: 0.5125\n",
      "Epoch 14/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9469 - sparse_categorical_accuracy: 0.5539 - val_loss: 1.0095 - val_sparse_categorical_accuracy: 0.5109\n",
      "Epoch 15/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9462 - sparse_categorical_accuracy: 0.5530 - val_loss: 1.0146 - val_sparse_categorical_accuracy: 0.5125\n",
      "Epoch 16/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9435 - sparse_categorical_accuracy: 0.5533 - val_loss: 1.0101 - val_sparse_categorical_accuracy: 0.5114\n",
      "Epoch 17/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9416 - sparse_categorical_accuracy: 0.5570 - val_loss: 1.0154 - val_sparse_categorical_accuracy: 0.5058\n",
      "Epoch 18/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9427 - sparse_categorical_accuracy: 0.5548 - val_loss: 1.0196 - val_sparse_categorical_accuracy: 0.5159\n",
      "Epoch 19/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9386 - sparse_categorical_accuracy: 0.5589 - val_loss: 1.0146 - val_sparse_categorical_accuracy: 0.5093\n",
      "Epoch 20/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9381 - sparse_categorical_accuracy: 0.5588 - val_loss: 1.0184 - val_sparse_categorical_accuracy: 0.5093\n",
      "Epoch 21/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9365 - sparse_categorical_accuracy: 0.5615 - val_loss: 1.0144 - val_sparse_categorical_accuracy: 0.5063\n",
      "Epoch 22/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9372 - sparse_categorical_accuracy: 0.5612 - val_loss: 1.0154 - val_sparse_categorical_accuracy: 0.5117\n",
      "Epoch 23/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9379 - sparse_categorical_accuracy: 0.5593 - val_loss: 1.0234 - val_sparse_categorical_accuracy: 0.5089\n",
      "Epoch 24/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9341 - sparse_categorical_accuracy: 0.5618 - val_loss: 1.0189 - val_sparse_categorical_accuracy: 0.5024\n",
      "Epoch 25/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9321 - sparse_categorical_accuracy: 0.5641 - val_loss: 1.0182 - val_sparse_categorical_accuracy: 0.5060\n",
      "Epoch 26/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9309 - sparse_categorical_accuracy: 0.5625 - val_loss: 1.0223 - val_sparse_categorical_accuracy: 0.5045\n",
      "Epoch 27/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9295 - sparse_categorical_accuracy: 0.5676 - val_loss: 1.0216 - val_sparse_categorical_accuracy: 0.4978\n",
      "Epoch 28/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9301 - sparse_categorical_accuracy: 0.5655 - val_loss: 1.0248 - val_sparse_categorical_accuracy: 0.5018\n",
      "Epoch 29/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9295 - sparse_categorical_accuracy: 0.5650 - val_loss: 1.0249 - val_sparse_categorical_accuracy: 0.5082\n",
      "Epoch 30/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9280 - sparse_categorical_accuracy: 0.5682 - val_loss: 1.0222 - val_sparse_categorical_accuracy: 0.5083\n",
      "Epoch 31/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9278 - sparse_categorical_accuracy: 0.5644 - val_loss: 1.0264 - val_sparse_categorical_accuracy: 0.4986\n",
      "Epoch 32/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9252 - sparse_categorical_accuracy: 0.5668 - val_loss: 1.0246 - val_sparse_categorical_accuracy: 0.5036\n",
      "Epoch 33/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9241 - sparse_categorical_accuracy: 0.5695 - val_loss: 1.0239 - val_sparse_categorical_accuracy: 0.5072\n",
      "Epoch 34/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9230 - sparse_categorical_accuracy: 0.5664 - val_loss: 1.0258 - val_sparse_categorical_accuracy: 0.5074\n",
      "Epoch 35/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9228 - sparse_categorical_accuracy: 0.5677 - val_loss: 1.0352 - val_sparse_categorical_accuracy: 0.5089\n",
      "Epoch 36/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9213 - sparse_categorical_accuracy: 0.5690 - val_loss: 1.0273 - val_sparse_categorical_accuracy: 0.5036\n",
      "Epoch 37/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9208 - sparse_categorical_accuracy: 0.5711 - val_loss: 1.0370 - val_sparse_categorical_accuracy: 0.4896\n",
      "Epoch 38/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9197 - sparse_categorical_accuracy: 0.5677 - val_loss: 1.0283 - val_sparse_categorical_accuracy: 0.5048\n",
      "Epoch 39/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9179 - sparse_categorical_accuracy: 0.5728 - val_loss: 1.0287 - val_sparse_categorical_accuracy: 0.5017\n",
      "Epoch 40/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9164 - sparse_categorical_accuracy: 0.5749 - val_loss: 1.0251 - val_sparse_categorical_accuracy: 0.5040\n",
      "Epoch 41/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9155 - sparse_categorical_accuracy: 0.5723 - val_loss: 1.0265 - val_sparse_categorical_accuracy: 0.5016\n",
      "Epoch 42/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9158 - sparse_categorical_accuracy: 0.5725 - val_loss: 1.0285 - val_sparse_categorical_accuracy: 0.5028\n",
      "Epoch 43/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.9147 - sparse_categorical_accuracy: 0.5739 - val_loss: 1.0339 - val_sparse_categorical_accuracy: 0.4978\n",
      "Epoch 44/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9139 - sparse_categorical_accuracy: 0.5736 - val_loss: 1.0311 - val_sparse_categorical_accuracy: 0.4948\n",
      "Epoch 45/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9118 - sparse_categorical_accuracy: 0.5726 - val_loss: 1.0369 - val_sparse_categorical_accuracy: 0.5033\n",
      "Epoch 46/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9108 - sparse_categorical_accuracy: 0.5745 - val_loss: 1.0332 - val_sparse_categorical_accuracy: 0.4938\n",
      "Epoch 47/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9099 - sparse_categorical_accuracy: 0.5737 - val_loss: 1.0355 - val_sparse_categorical_accuracy: 0.4987\n",
      "Epoch 48/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9082 - sparse_categorical_accuracy: 0.5786 - val_loss: 1.0315 - val_sparse_categorical_accuracy: 0.4993\n",
      "Epoch 49/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9085 - sparse_categorical_accuracy: 0.5770 - val_loss: 1.0375 - val_sparse_categorical_accuracy: 0.4963\n",
      "Epoch 50/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9086 - sparse_categorical_accuracy: 0.5769 - val_loss: 1.0382 - val_sparse_categorical_accuracy: 0.4906\n",
      "Epoch 51/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9081 - sparse_categorical_accuracy: 0.5760 - val_loss: 1.0334 - val_sparse_categorical_accuracy: 0.4972\n",
      "Epoch 52/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9047 - sparse_categorical_accuracy: 0.5787 - val_loss: 1.0445 - val_sparse_categorical_accuracy: 0.4953\n",
      "Epoch 53/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9043 - sparse_categorical_accuracy: 0.5801 - val_loss: 1.0392 - val_sparse_categorical_accuracy: 0.5025\n",
      "Epoch 54/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9020 - sparse_categorical_accuracy: 0.5808 - val_loss: 1.0404 - val_sparse_categorical_accuracy: 0.4971\n",
      "Epoch 55/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9018 - sparse_categorical_accuracy: 0.5787 - val_loss: 1.0455 - val_sparse_categorical_accuracy: 0.4995\n",
      "Epoch 56/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9014 - sparse_categorical_accuracy: 0.5802 - val_loss: 1.0321 - val_sparse_categorical_accuracy: 0.5001\n",
      "Epoch 57/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.9030 - sparse_categorical_accuracy: 0.5814 - val_loss: 1.0393 - val_sparse_categorical_accuracy: 0.4903\n",
      "Epoch 58/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8973 - sparse_categorical_accuracy: 0.5839 - val_loss: 1.0394 - val_sparse_categorical_accuracy: 0.5028\n",
      "Epoch 59/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8992 - sparse_categorical_accuracy: 0.5841 - val_loss: 1.0367 - val_sparse_categorical_accuracy: 0.4993\n",
      "Epoch 60/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8985 - sparse_categorical_accuracy: 0.5851 - val_loss: 1.0427 - val_sparse_categorical_accuracy: 0.4922\n",
      "Epoch 61/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8944 - sparse_categorical_accuracy: 0.5880 - val_loss: 1.0460 - val_sparse_categorical_accuracy: 0.4907\n",
      "Epoch 62/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8972 - sparse_categorical_accuracy: 0.5825 - val_loss: 1.0535 - val_sparse_categorical_accuracy: 0.4882\n",
      "Epoch 63/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8934 - sparse_categorical_accuracy: 0.5842 - val_loss: 1.0453 - val_sparse_categorical_accuracy: 0.5032\n",
      "Epoch 64/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8935 - sparse_categorical_accuracy: 0.5853 - val_loss: 1.0395 - val_sparse_categorical_accuracy: 0.4994\n",
      "Epoch 65/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8905 - sparse_categorical_accuracy: 0.5861 - val_loss: 1.0496 - val_sparse_categorical_accuracy: 0.4945\n",
      "Epoch 66/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8902 - sparse_categorical_accuracy: 0.5885 - val_loss: 1.0568 - val_sparse_categorical_accuracy: 0.4848\n",
      "Epoch 67/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8885 - sparse_categorical_accuracy: 0.5913 - val_loss: 1.0566 - val_sparse_categorical_accuracy: 0.4971\n",
      "Epoch 68/200\n",
      "347/347 [==============================] - 4s 13ms/step - loss: 0.8899 - sparse_categorical_accuracy: 0.5903 - val_loss: 1.0481 - val_sparse_categorical_accuracy: 0.4934\n",
      "Epoch 69/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8907 - sparse_categorical_accuracy: 0.5880 - val_loss: 1.0480 - val_sparse_categorical_accuracy: 0.4869\n",
      "Epoch 70/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8875 - sparse_categorical_accuracy: 0.5867 - val_loss: 1.0472 - val_sparse_categorical_accuracy: 0.4914\n",
      "Epoch 71/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8864 - sparse_categorical_accuracy: 0.5891 - val_loss: 1.0457 - val_sparse_categorical_accuracy: 0.4998\n",
      "Epoch 72/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8855 - sparse_categorical_accuracy: 0.5905 - val_loss: 1.0565 - val_sparse_categorical_accuracy: 0.4842\n",
      "Epoch 73/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8838 - sparse_categorical_accuracy: 0.5935 - val_loss: 1.0457 - val_sparse_categorical_accuracy: 0.4952\n",
      "Epoch 74/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8851 - sparse_categorical_accuracy: 0.5913 - val_loss: 1.0553 - val_sparse_categorical_accuracy: 0.4901\n",
      "Epoch 75/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8849 - sparse_categorical_accuracy: 0.5941 - val_loss: 1.0674 - val_sparse_categorical_accuracy: 0.4796\n",
      "Epoch 76/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8813 - sparse_categorical_accuracy: 0.5955 - val_loss: 1.0512 - val_sparse_categorical_accuracy: 0.4945\n",
      "Epoch 77/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8781 - sparse_categorical_accuracy: 0.5982 - val_loss: 1.0535 - val_sparse_categorical_accuracy: 0.4918\n",
      "Epoch 78/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8820 - sparse_categorical_accuracy: 0.5932 - val_loss: 1.0681 - val_sparse_categorical_accuracy: 0.4761\n",
      "Epoch 79/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8794 - sparse_categorical_accuracy: 0.5948 - val_loss: 1.0683 - val_sparse_categorical_accuracy: 0.4997\n",
      "Epoch 80/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8797 - sparse_categorical_accuracy: 0.5915 - val_loss: 1.0547 - val_sparse_categorical_accuracy: 0.4933\n",
      "Epoch 81/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8778 - sparse_categorical_accuracy: 0.5967 - val_loss: 1.0604 - val_sparse_categorical_accuracy: 0.4769\n",
      "Epoch 82/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8765 - sparse_categorical_accuracy: 0.5966 - val_loss: 1.0568 - val_sparse_categorical_accuracy: 0.4831\n",
      "Epoch 83/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8739 - sparse_categorical_accuracy: 0.5971 - val_loss: 1.0575 - val_sparse_categorical_accuracy: 0.4955\n",
      "Epoch 84/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8759 - sparse_categorical_accuracy: 0.5961 - val_loss: 1.0683 - val_sparse_categorical_accuracy: 0.4963\n",
      "Epoch 85/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8745 - sparse_categorical_accuracy: 0.5968 - val_loss: 1.0642 - val_sparse_categorical_accuracy: 0.4967\n",
      "Epoch 86/200\n",
      "347/347 [==============================] - 5s 14ms/step - loss: 0.8740 - sparse_categorical_accuracy: 0.5969 - val_loss: 1.0656 - val_sparse_categorical_accuracy: 0.4890\n",
      "Epoch 87/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8721 - sparse_categorical_accuracy: 0.5961 - val_loss: 1.0614 - val_sparse_categorical_accuracy: 0.4829\n",
      "Epoch 88/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8701 - sparse_categorical_accuracy: 0.5988 - val_loss: 1.0657 - val_sparse_categorical_accuracy: 0.4959\n",
      "Epoch 89/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8697 - sparse_categorical_accuracy: 0.5999 - val_loss: 1.0681 - val_sparse_categorical_accuracy: 0.4933\n",
      "Epoch 90/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8705 - sparse_categorical_accuracy: 0.6005 - val_loss: 1.0728 - val_sparse_categorical_accuracy: 0.4822\n",
      "Epoch 91/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8655 - sparse_categorical_accuracy: 0.6033 - val_loss: 1.0741 - val_sparse_categorical_accuracy: 0.4750\n",
      "Epoch 92/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8654 - sparse_categorical_accuracy: 0.6014 - val_loss: 1.0679 - val_sparse_categorical_accuracy: 0.4880\n",
      "Epoch 93/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8652 - sparse_categorical_accuracy: 0.6020 - val_loss: 1.0655 - val_sparse_categorical_accuracy: 0.4906\n",
      "Epoch 94/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8660 - sparse_categorical_accuracy: 0.6016 - val_loss: 1.0681 - val_sparse_categorical_accuracy: 0.4783\n",
      "Epoch 95/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8634 - sparse_categorical_accuracy: 0.6029 - val_loss: 1.0773 - val_sparse_categorical_accuracy: 0.4765\n",
      "Epoch 96/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8594 - sparse_categorical_accuracy: 0.6057 - val_loss: 1.0657 - val_sparse_categorical_accuracy: 0.4884\n",
      "Epoch 97/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8639 - sparse_categorical_accuracy: 0.6019 - val_loss: 1.0668 - val_sparse_categorical_accuracy: 0.4886\n",
      "Epoch 98/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8618 - sparse_categorical_accuracy: 0.6053 - val_loss: 1.0778 - val_sparse_categorical_accuracy: 0.4965\n",
      "Epoch 99/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8597 - sparse_categorical_accuracy: 0.6047 - val_loss: 1.0743 - val_sparse_categorical_accuracy: 0.4830\n",
      "Epoch 100/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8585 - sparse_categorical_accuracy: 0.6052 - val_loss: 1.0729 - val_sparse_categorical_accuracy: 0.4845\n",
      "Epoch 101/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8618 - sparse_categorical_accuracy: 0.6043 - val_loss: 1.0767 - val_sparse_categorical_accuracy: 0.4884\n",
      "Epoch 102/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8570 - sparse_categorical_accuracy: 0.6057 - val_loss: 1.0764 - val_sparse_categorical_accuracy: 0.4891\n",
      "Epoch 103/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8574 - sparse_categorical_accuracy: 0.6067 - val_loss: 1.0698 - val_sparse_categorical_accuracy: 0.4841\n",
      "Epoch 104/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8569 - sparse_categorical_accuracy: 0.6058 - val_loss: 1.0824 - val_sparse_categorical_accuracy: 0.4848\n",
      "Epoch 105/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8543 - sparse_categorical_accuracy: 0.6061 - val_loss: 1.0863 - val_sparse_categorical_accuracy: 0.4742\n",
      "Epoch 106/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8551 - sparse_categorical_accuracy: 0.6084 - val_loss: 1.0869 - val_sparse_categorical_accuracy: 0.4830\n",
      "Epoch 107/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8537 - sparse_categorical_accuracy: 0.6096 - val_loss: 1.0748 - val_sparse_categorical_accuracy: 0.4838\n",
      "Epoch 108/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8478 - sparse_categorical_accuracy: 0.6166 - val_loss: 1.0782 - val_sparse_categorical_accuracy: 0.4811\n",
      "Epoch 109/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8531 - sparse_categorical_accuracy: 0.6100 - val_loss: 1.0813 - val_sparse_categorical_accuracy: 0.4825\n",
      "Epoch 110/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8514 - sparse_categorical_accuracy: 0.6100 - val_loss: 1.0881 - val_sparse_categorical_accuracy: 0.4919\n",
      "Epoch 111/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8480 - sparse_categorical_accuracy: 0.6147 - val_loss: 1.0881 - val_sparse_categorical_accuracy: 0.4834\n",
      "Epoch 112/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8480 - sparse_categorical_accuracy: 0.6147 - val_loss: 1.0943 - val_sparse_categorical_accuracy: 0.4848\n",
      "Epoch 113/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8457 - sparse_categorical_accuracy: 0.6146 - val_loss: 1.0900 - val_sparse_categorical_accuracy: 0.4875\n",
      "Epoch 114/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8464 - sparse_categorical_accuracy: 0.6146 - val_loss: 1.0934 - val_sparse_categorical_accuracy: 0.4844\n",
      "Epoch 115/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8450 - sparse_categorical_accuracy: 0.6125 - val_loss: 1.0893 - val_sparse_categorical_accuracy: 0.4798\n",
      "Epoch 116/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8466 - sparse_categorical_accuracy: 0.6120 - val_loss: 1.0857 - val_sparse_categorical_accuracy: 0.4844\n",
      "Epoch 117/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8448 - sparse_categorical_accuracy: 0.6126 - val_loss: 1.0893 - val_sparse_categorical_accuracy: 0.4802\n",
      "Epoch 118/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8461 - sparse_categorical_accuracy: 0.6122 - val_loss: 1.0874 - val_sparse_categorical_accuracy: 0.4792\n",
      "Epoch 119/200\n",
      "347/347 [==============================] - 4s 13ms/step - loss: 0.8439 - sparse_categorical_accuracy: 0.6186 - val_loss: 1.0981 - val_sparse_categorical_accuracy: 0.4813\n",
      "Epoch 120/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8415 - sparse_categorical_accuracy: 0.6156 - val_loss: 1.1000 - val_sparse_categorical_accuracy: 0.4876\n",
      "Epoch 121/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8431 - sparse_categorical_accuracy: 0.6156 - val_loss: 1.0875 - val_sparse_categorical_accuracy: 0.4890\n",
      "Epoch 122/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8385 - sparse_categorical_accuracy: 0.6187 - val_loss: 1.0993 - val_sparse_categorical_accuracy: 0.4880\n",
      "Epoch 123/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8408 - sparse_categorical_accuracy: 0.6163 - val_loss: 1.1033 - val_sparse_categorical_accuracy: 0.4791\n",
      "Epoch 124/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8394 - sparse_categorical_accuracy: 0.6184 - val_loss: 1.0885 - val_sparse_categorical_accuracy: 0.4803\n",
      "Epoch 125/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8347 - sparse_categorical_accuracy: 0.6204 - val_loss: 1.0990 - val_sparse_categorical_accuracy: 0.4806\n",
      "Epoch 126/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8375 - sparse_categorical_accuracy: 0.6186 - val_loss: 1.0990 - val_sparse_categorical_accuracy: 0.4825\n",
      "Epoch 127/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8384 - sparse_categorical_accuracy: 0.6202 - val_loss: 1.0977 - val_sparse_categorical_accuracy: 0.4899\n",
      "Epoch 128/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8349 - sparse_categorical_accuracy: 0.6221 - val_loss: 1.0991 - val_sparse_categorical_accuracy: 0.4806\n",
      "Epoch 129/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8321 - sparse_categorical_accuracy: 0.6183 - val_loss: 1.1042 - val_sparse_categorical_accuracy: 0.4840\n",
      "Epoch 130/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8307 - sparse_categorical_accuracy: 0.6194 - val_loss: 1.1172 - val_sparse_categorical_accuracy: 0.4869\n",
      "Epoch 131/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8358 - sparse_categorical_accuracy: 0.6178 - val_loss: 1.0980 - val_sparse_categorical_accuracy: 0.4875\n",
      "Epoch 132/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8325 - sparse_categorical_accuracy: 0.6223 - val_loss: 1.1122 - val_sparse_categorical_accuracy: 0.4669\n",
      "Epoch 133/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8303 - sparse_categorical_accuracy: 0.6219 - val_loss: 1.1098 - val_sparse_categorical_accuracy: 0.4865\n",
      "Epoch 134/200\n",
      "347/347 [==============================] - 5s 13ms/step - loss: 0.8277 - sparse_categorical_accuracy: 0.6234 - val_loss: 1.1063 - val_sparse_categorical_accuracy: 0.4775\n",
      "Epoch 135/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8294 - sparse_categorical_accuracy: 0.6226 - val_loss: 1.0993 - val_sparse_categorical_accuracy: 0.4852\n",
      "Epoch 136/200\n",
      "347/347 [==============================] - 4s 13ms/step - loss: 0.8265 - sparse_categorical_accuracy: 0.6257 - val_loss: 1.0976 - val_sparse_categorical_accuracy: 0.4865\n",
      "Epoch 137/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8275 - sparse_categorical_accuracy: 0.6200 - val_loss: 1.1025 - val_sparse_categorical_accuracy: 0.4829\n",
      "Epoch 138/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8282 - sparse_categorical_accuracy: 0.6261 - val_loss: 1.1081 - val_sparse_categorical_accuracy: 0.4810\n",
      "Epoch 139/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8242 - sparse_categorical_accuracy: 0.6217 - val_loss: 1.1126 - val_sparse_categorical_accuracy: 0.4844\n",
      "Epoch 140/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8223 - sparse_categorical_accuracy: 0.6246 - val_loss: 1.1047 - val_sparse_categorical_accuracy: 0.4773\n",
      "Epoch 141/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8235 - sparse_categorical_accuracy: 0.6263 - val_loss: 1.1263 - val_sparse_categorical_accuracy: 0.4775\n",
      "Epoch 142/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8225 - sparse_categorical_accuracy: 0.6276 - val_loss: 1.1180 - val_sparse_categorical_accuracy: 0.4776\n",
      "Epoch 143/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8264 - sparse_categorical_accuracy: 0.6234 - val_loss: 1.1251 - val_sparse_categorical_accuracy: 0.4779\n",
      "Epoch 144/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8205 - sparse_categorical_accuracy: 0.6274 - val_loss: 1.1310 - val_sparse_categorical_accuracy: 0.4733\n",
      "Epoch 145/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8216 - sparse_categorical_accuracy: 0.6280 - val_loss: 1.1184 - val_sparse_categorical_accuracy: 0.4779\n",
      "Epoch 146/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8167 - sparse_categorical_accuracy: 0.6280 - val_loss: 1.1291 - val_sparse_categorical_accuracy: 0.4802\n",
      "Epoch 147/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8181 - sparse_categorical_accuracy: 0.6326 - val_loss: 1.1221 - val_sparse_categorical_accuracy: 0.4743\n",
      "Epoch 148/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8140 - sparse_categorical_accuracy: 0.6305 - val_loss: 1.1214 - val_sparse_categorical_accuracy: 0.4869\n",
      "Epoch 149/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8186 - sparse_categorical_accuracy: 0.6293 - val_loss: 1.1250 - val_sparse_categorical_accuracy: 0.4769\n",
      "Epoch 150/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8164 - sparse_categorical_accuracy: 0.6340 - val_loss: 1.1263 - val_sparse_categorical_accuracy: 0.4756\n",
      "Epoch 151/200\n",
      "347/347 [==============================] - 4s 13ms/step - loss: 0.8177 - sparse_categorical_accuracy: 0.6284 - val_loss: 1.1340 - val_sparse_categorical_accuracy: 0.4734\n",
      "Epoch 152/200\n",
      "347/347 [==============================] - 5s 13ms/step - loss: 0.8153 - sparse_categorical_accuracy: 0.6300 - val_loss: 1.1312 - val_sparse_categorical_accuracy: 0.4697\n",
      "Epoch 153/200\n",
      "347/347 [==============================] - 5s 13ms/step - loss: 0.8145 - sparse_categorical_accuracy: 0.6350 - val_loss: 1.1266 - val_sparse_categorical_accuracy: 0.4803\n",
      "Epoch 154/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8156 - sparse_categorical_accuracy: 0.6337 - val_loss: 1.1273 - val_sparse_categorical_accuracy: 0.4798\n",
      "Epoch 155/200\n",
      "347/347 [==============================] - 4s 13ms/step - loss: 0.8155 - sparse_categorical_accuracy: 0.6280 - val_loss: 1.1238 - val_sparse_categorical_accuracy: 0.4764\n",
      "Epoch 156/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8138 - sparse_categorical_accuracy: 0.6329 - val_loss: 1.1131 - val_sparse_categorical_accuracy: 0.4853\n",
      "Epoch 157/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8104 - sparse_categorical_accuracy: 0.6343 - val_loss: 1.1422 - val_sparse_categorical_accuracy: 0.4776\n",
      "Epoch 158/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8129 - sparse_categorical_accuracy: 0.6346 - val_loss: 1.1381 - val_sparse_categorical_accuracy: 0.4906\n",
      "Epoch 159/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8106 - sparse_categorical_accuracy: 0.6344 - val_loss: 1.1356 - val_sparse_categorical_accuracy: 0.4798\n",
      "Epoch 160/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8070 - sparse_categorical_accuracy: 0.6361 - val_loss: 1.1278 - val_sparse_categorical_accuracy: 0.4712\n",
      "Epoch 161/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8104 - sparse_categorical_accuracy: 0.6321 - val_loss: 1.1431 - val_sparse_categorical_accuracy: 0.4764\n",
      "Epoch 162/200\n",
      "347/347 [==============================] - 5s 13ms/step - loss: 0.8065 - sparse_categorical_accuracy: 0.6344 - val_loss: 1.1259 - val_sparse_categorical_accuracy: 0.4810\n",
      "Epoch 163/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8053 - sparse_categorical_accuracy: 0.6353 - val_loss: 1.1243 - val_sparse_categorical_accuracy: 0.4757\n",
      "Epoch 164/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8073 - sparse_categorical_accuracy: 0.6300 - val_loss: 1.1404 - val_sparse_categorical_accuracy: 0.4742\n",
      "Epoch 165/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8078 - sparse_categorical_accuracy: 0.6334 - val_loss: 1.1277 - val_sparse_categorical_accuracy: 0.4807\n",
      "Epoch 166/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8076 - sparse_categorical_accuracy: 0.6344 - val_loss: 1.1393 - val_sparse_categorical_accuracy: 0.4802\n",
      "Epoch 167/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.8038 - sparse_categorical_accuracy: 0.6372 - val_loss: 1.1279 - val_sparse_categorical_accuracy: 0.4804\n",
      "Epoch 168/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8027 - sparse_categorical_accuracy: 0.6377 - val_loss: 1.1504 - val_sparse_categorical_accuracy: 0.4785\n",
      "Epoch 169/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8031 - sparse_categorical_accuracy: 0.6360 - val_loss: 1.1412 - val_sparse_categorical_accuracy: 0.4684\n",
      "Epoch 170/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8013 - sparse_categorical_accuracy: 0.6346 - val_loss: 1.1417 - val_sparse_categorical_accuracy: 0.4849\n",
      "Epoch 171/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7977 - sparse_categorical_accuracy: 0.6396 - val_loss: 1.1554 - val_sparse_categorical_accuracy: 0.4688\n",
      "Epoch 172/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.7987 - sparse_categorical_accuracy: 0.6382 - val_loss: 1.1338 - val_sparse_categorical_accuracy: 0.4792\n",
      "Epoch 173/200\n",
      "347/347 [==============================] - 4s 13ms/step - loss: 0.7993 - sparse_categorical_accuracy: 0.6409 - val_loss: 1.1323 - val_sparse_categorical_accuracy: 0.4757\n",
      "Epoch 174/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.8008 - sparse_categorical_accuracy: 0.6382 - val_loss: 1.1476 - val_sparse_categorical_accuracy: 0.4798\n",
      "Epoch 175/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7966 - sparse_categorical_accuracy: 0.6399 - val_loss: 1.1514 - val_sparse_categorical_accuracy: 0.4641\n",
      "Epoch 176/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.7973 - sparse_categorical_accuracy: 0.6378 - val_loss: 1.1373 - val_sparse_categorical_accuracy: 0.4799\n",
      "Epoch 177/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.7931 - sparse_categorical_accuracy: 0.6388 - val_loss: 1.1594 - val_sparse_categorical_accuracy: 0.4696\n",
      "Epoch 178/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7952 - sparse_categorical_accuracy: 0.6393 - val_loss: 1.1598 - val_sparse_categorical_accuracy: 0.4739\n",
      "Epoch 179/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.7933 - sparse_categorical_accuracy: 0.6425 - val_loss: 1.1612 - val_sparse_categorical_accuracy: 0.4822\n",
      "Epoch 180/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7919 - sparse_categorical_accuracy: 0.6431 - val_loss: 1.1496 - val_sparse_categorical_accuracy: 0.4750\n",
      "Epoch 181/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7959 - sparse_categorical_accuracy: 0.6411 - val_loss: 1.1564 - val_sparse_categorical_accuracy: 0.4779\n",
      "Epoch 182/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7889 - sparse_categorical_accuracy: 0.6468 - val_loss: 1.1490 - val_sparse_categorical_accuracy: 0.4788\n",
      "Epoch 183/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7860 - sparse_categorical_accuracy: 0.6452 - val_loss: 1.1685 - val_sparse_categorical_accuracy: 0.4678\n",
      "Epoch 184/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7922 - sparse_categorical_accuracy: 0.6456 - val_loss: 1.1661 - val_sparse_categorical_accuracy: 0.4762\n",
      "Epoch 185/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7843 - sparse_categorical_accuracy: 0.6497 - val_loss: 1.1823 - val_sparse_categorical_accuracy: 0.4557\n",
      "Epoch 186/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7889 - sparse_categorical_accuracy: 0.6399 - val_loss: 1.1492 - val_sparse_categorical_accuracy: 0.4711\n",
      "Epoch 187/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7932 - sparse_categorical_accuracy: 0.6422 - val_loss: 1.1452 - val_sparse_categorical_accuracy: 0.4753\n",
      "Epoch 188/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7873 - sparse_categorical_accuracy: 0.6479 - val_loss: 1.1540 - val_sparse_categorical_accuracy: 0.4806\n",
      "Epoch 189/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7915 - sparse_categorical_accuracy: 0.6413 - val_loss: 1.1662 - val_sparse_categorical_accuracy: 0.4753\n",
      "Epoch 190/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7861 - sparse_categorical_accuracy: 0.6456 - val_loss: 1.1659 - val_sparse_categorical_accuracy: 0.4673\n",
      "Epoch 191/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7875 - sparse_categorical_accuracy: 0.6437 - val_loss: 1.1783 - val_sparse_categorical_accuracy: 0.4777\n",
      "Epoch 192/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7885 - sparse_categorical_accuracy: 0.6441 - val_loss: 1.1649 - val_sparse_categorical_accuracy: 0.4814\n",
      "Epoch 193/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7865 - sparse_categorical_accuracy: 0.6469 - val_loss: 1.1512 - val_sparse_categorical_accuracy: 0.4720\n",
      "Epoch 194/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7833 - sparse_categorical_accuracy: 0.6477 - val_loss: 1.1589 - val_sparse_categorical_accuracy: 0.4750\n",
      "Epoch 195/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7782 - sparse_categorical_accuracy: 0.6525 - val_loss: 1.1654 - val_sparse_categorical_accuracy: 0.4716\n",
      "Epoch 196/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.7806 - sparse_categorical_accuracy: 0.6461 - val_loss: 1.1561 - val_sparse_categorical_accuracy: 0.4715\n",
      "Epoch 197/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.7852 - sparse_categorical_accuracy: 0.6438 - val_loss: 1.1647 - val_sparse_categorical_accuracy: 0.4760\n",
      "Epoch 198/200\n",
      "347/347 [==============================] - 4s 11ms/step - loss: 0.7831 - sparse_categorical_accuracy: 0.6486 - val_loss: 1.1672 - val_sparse_categorical_accuracy: 0.4745\n",
      "Epoch 199/200\n",
      "347/347 [==============================] - 4s 13ms/step - loss: 0.7858 - sparse_categorical_accuracy: 0.6398 - val_loss: 1.1626 - val_sparse_categorical_accuracy: 0.4799\n",
      "Epoch 200/200\n",
      "347/347 [==============================] - 4s 12ms/step - loss: 0.7804 - sparse_categorical_accuracy: 0.6467 - val_loss: 1.1821 - val_sparse_categorical_accuracy: 0.4687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x188115790>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class_weights={0:1,1:0.5,2:1}\n",
    "model_noFT.fit(\n",
    "    train_data,\n",
    "    epochs=200,\n",
    "    validation_data=test_data,\n",
    "    #class_weight=class_weights,\n",
    "    callbacks=[WandbCallback()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Adam(0.0003)\n",
    "    \n",
    "# 1er essai\n",
    "\n",
    "home_input = tf.keras.Input(shape = (10,6), name = 'home_train_input')  \n",
    "away_input = tf.keras.Input(shape = (10,6), name = 'away_train_input')  \n",
    "LSTM_output = base_LSTM_model([home_input,away_input])  \n",
    "\n",
    "player_input = tf.keras.Input(shape = (198), name = 'player_train_input')  \n",
    "concat_layer = tf.keras.layers.Concatenate(name='concat_layer')([LSTM_output,player_input])  \n",
    "Dense1 = tf.keras.layers.Dense(256,'relu',name = 'Dense1')(concat_layer)  \n",
    "Dense1 = tf.keras.layers.Dropout(0.234375)(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dense(128,'relu',name='Dense2')(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dropout(0.125)(Dense2)  \n",
    "Dense_output = tf.keras.layers.Dense(3, 'softmax', name='output')(Dense2)   \n",
    "\n",
    "    val_loss overfit à 170/300 epoch à 0.98 => train_acc = 0.546 et val_acc = 0.532\n",
    "    val_acc => 0.535 à 238/300\n",
    "\n",
    "# 2e essai\n",
    "1er essai + class_weights={0:1,1:10,2:1}  \n",
    "\n",
    "    Horrible, fout une merde pas possible\n",
    "# 3e essai\n",
    "1er essai + class_weights={0:1,1:0.5,2:1}  \n",
    "\n",
    "    Overfitte immédiatement\n",
    "\n",
    "# 4e essai\n",
    "\n",
    "home_input = tf.keras.Input(shape = (10,6), name = 'home_train_input')  \n",
    "away_input = tf.keras.Input(shape = (10,6), name = 'away_train_input')  \n",
    "LSTM_output = base_LSTM_model([home_input,away_input])  \n",
    "\n",
    "player_input = tf.keras.Input(shape = (198), name = 'player_train_input')  \n",
    "concat_layer = tf.keras.layers.Concatenate(name='concat_layer')([LSTM_output,player_input])  \n",
    "Dense1 = tf.keras.layers.Dense(256,'relu',name = 'Dense1')(concat_layer)  \n",
    "Dense1 = tf.keras.layers.Dropout(0.234375)(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dense(128,'relu',name='Dense2')(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dropout(0.2)(Dense2)  \n",
    "Dense3 = tf.keras.layers.Dense(32,'relu',name='Dense3')(Dense2)  \n",
    "Dense3 = tf.keras.layers.Dropout(0.125)(Dense3)  \n",
    "Dense_output = tf.keras.layers.Dense(3, 'softmax', name='output')(Dense2)  \n",
    "\n",
    "    overfit ~168 epoch => val_acc = 0.532-0.536 et train_acc = 0.544\n",
    "\n",
    "# 5e essai\n",
    "1e essai + full trainable   \n",
    "\n",
    "    train loss diminue bcp plus vite mais val loss augmente. val accuracy reste stable sur 100 epochs\n",
    "    ça overfit carrément\n",
    "\n",
    "# 6e essai\n",
    "1e essai + BatchNormalization après Dense1()\n",
    "\n",
    "    Pareil ça overfit immédiatement\n",
    "\n",
    "# 7e essai\n",
    "1e essai + BatchNormalization après LSTM()\n",
    "\n",
    "    Pas mal! ça overfitte après 220-240 epochs avec val_acc = 0.531 et train_acc = 0.546\n",
    "    En plus ça prédit pas mal les Draw\n",
    "\n",
    "# 8\n",
    "    cette fois on met le modele LSTM en entier avec la dernière couche softmax\n",
    "\n",
    "home_input = tf.keras.Input(shape = (10,6), name = 'home_train_input')  \n",
    "away_input = tf.keras.Input(shape = (10,6), name = 'away_train_input')  \n",
    "LSTM_output = base_LSTM_model([home_input,away_input])  \n",
    "LSTM_output = tf.keras.layers.Dropout(0.34)(LSTM_output)\n",
    "\n",
    "player_input = tf.keras.Input(shape = (198), name = 'player_train_input')  \n",
    "concat_layer = tf.keras.layers.Concatenate(name='concat_layer')([LSTM_output,player_input])  \n",
    "Dense1 = tf.keras.layers.Dense(256,'relu',name = 'Dense1')(concat_layer)  \n",
    "Dense1 = tf.keras.layers.Dropout(0.234375)(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dense(128,'relu',name='Dense2')(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dropout(0.125)(Dense2)  \n",
    "Dense_output = tf.keras.layers.Dense(3, 'softmax', name='output')(Dense2)   \n",
    "\n",
    "    assez stable au niveau de l'overfitting. ça overfitte au bout de 300 epochs mais ça donne pas des résultats fous: val_acc = 0.53, train_acc = 0.545\n",
    "\n",
    "# 9\n",
    "    meme chose mais on rend la dernière couche de LSTM (softmax) trainable.\n",
    "    bons résultats overfitte au bout de 200 epochs avec val_acc = 0.532 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5470638009815536\n",
      "Recall: \n",
      " {0: 0.8225721389286521, 1: 0.0033356497567755385, 2: 0.5751900407623665} \n",
      "Precision: \n",
      " {0: 0.5599261500589774, 1: 0.5106382978723404, 2: 0.5221522152215221} \n",
      "F1-score: \n",
      " {0: 0.6663005004271939, 1: 0.006628003314001657, 2: 0.5473893898091844}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model_noFT.predict([train_home, train_away, X]).argmax(axis=-1)\n",
    "cm_metrics(y,y_pred, [0,1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5312034655475836\n",
      "Recall: \n",
      " {0: 0.8035552877372703, 1: 0.0011117287381878821, 2: 0.5531070956368445} \n",
      "Precision: \n",
      " {0: 0.5512608515915668, 1: 0.16666666666666666, 2: 0.4946787544343713} \n",
      "F1-score: \n",
      " {0: 0.6539168812063259, 1: 0.0022087244616234127, 2: 0.5222638368705785}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEGCAYAAAAT05LOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAj/ElEQVR4nO3deXxV1bn/8c+TEJIQ5lGEICigBRREBOuAIlaR2qr1OtVaf62teqvWXodeO1irXm+9va2ttldbqlRsrVy9oqJSBa2KI4MUlUFkHiMQQiCEBMjJ8/vj7MQAGc6BHPbJ2d/367Vf2WedPTz7lD6utdfea5m7IyISNVlhByAiEgYlPxGJJCU/EYkkJT8RiSQlPxGJpFZhB1BX187Z3rcwJ+ww0tanH7UJO4S05x30GzWmcudW9uwut4M5xjljCnxLSSyhbT/4aNcr7j7uYM6XKmmV/PoW5jD7lcKww0hb5xw+LOwQ0l7l6SPDDiGtzX/zgYM+RnFJjFmv9E5o25yey7se9AlTJK2Sn4i0BE7Mq8MO4qAp+YlIUhyopuW/HKHkJyJJq0Y1PxGJGMfZo2aviESNAzE1e0UkinTPT0Qix4FYBowGpeQnIklr+Xf8lPxEJEmOZ8Q9P73bKyJJcYc9CS6NMbNCM3vdzBaZ2UIzuyko/7mZrTez+cEyvs4+PzKzZWa2xMzOqVM+LihbZma3J3IdqvmJSJKMGAf1enCNKuAWd59nZu2AD8xsRvDdb9z9V3ud1WwQcBkwGDgceNXMBgZf/w/wJWAdMMfMprr7osZOruQnIklxoLoZWr3uXgQUBetlZrYY6NXILucDk919F7DSzJYBNS9zL3P3FQBmNjnYttHkp2aviCQtFtT+mlqArmY2t85yTX3HM7O+wPHArKDoBjP7yMwmmlmnoKwXsLbObuuCsobKG6Wan4gkJf6Qc8LN3mJ3H9HYBmbWFngG+IG7bzezh4F7glPdA/wa+PaBR1w/JT8RSYoDe7x5Go1mlkM88T3h7lMA3H1jne//BLwYfFwP1B3zrndQRiPlDVKzV0SS4hgxshJaGmNmBjwKLHb3++uU96yz2YXAgmB9KnCZmeWaWT9gADAbmAMMMLN+ZtaaeKfI1KauQzU/EUlatTdLb+8pwJXAx2Y2Pyj7MXC5mQ0jXslcBVwL4O4Lzewp4h0ZVcD17h4DMLMbgFeAbGCiuy9s6uRKfiKSlCTv+TV8HPe3od4DTWtkn3uBe+spn9bYfvVR8hORJBmxZrrnFyYlPxFJSnwkZyU/EYkYd2O3Z4cdxkFT8hORpFU3z+ttoVLyE5GkxDs81OwVkchRh4eIRJA6PEQksmLN85BzqJT8RCQpjrHHW37qaPlXICKHlDo8RCSSHFOzV0SiSR0eLcym9Tn89019KN2cA+aM/8YWLvxOMQDPP9qVqY91JSvbGTV2O9+5owiAFYvyePDfCykvyyIrC3437VOqqoxbLhhQe9ziohzOvGgr/3p3k0OIZYxJsxZRsSOb6mqIVRk3njuw6Z0yUNv8Xdx25Vv061UCbvzX46NZuKIHAJec9RHXXzyLr958JdvK87js7A85a+QyALKznCN6lnL+Ld+gbGdemJeQNHf0qEtTzGwc8ADxYWYecff7Unm+pmS3cq752QYGHFfBzh1Z3DBuIMNHl7F1cw7vvtKBh19dQutcp7Q4/rPEquCXNx7BbQ+u5qjBlWwvySY7x2md5zz86pLa415/zkBOHV8a0lWF54cXH8X2kkj993M/N176HrMX9ubOCWfRKjtGXusqALp12sGJg9bx2Za2tdtOnj6UydOHAnDycau5eOzHLS7xQU2HR8t/vS1l6dvMsonPqHQuMIj4GF2DUnW+RHTpUcWA4yoAaNO2msL+uyguyuHFx7tw6Q0baZ0bn5WlY9f4P+AP3mxHvy9UcNTgSgDad46Rvc//5uuW51Ja3Ioho8oP3YVIWijI283QAUW89M7RAFTFstlRkQvADRe/zx+mjMIbmOhn7InLeW1O/0MVarNrjsFMw5bK6EYSzKjk7ruBmhmV0sJna1uzfEE+xwzfyfrleSyY1Zbvf3kAt36tP0vm5wOwbkUeZvDjy4/k+rMH8tT/dN/vOG8835HTv1qKtfz7v8lx4z+fXMHvX/6Uc6/YEnY0oejZtYzSsnxuv+pNHvnJFG67ciZ5rfdwytBVFJe2Yfm6LvXul5tTxcjB63hzXt9DG3AzcYxqT2xJZ6lss9Q3o9KoFJ4vYRXlWdzznb5cd/d6CtpVE4tBWWk2D7y4lCXz23DvtX2Z9P5iYlWwYHYBv5v2Kbn51dx+aX8GHLeT40/bUXusN5/vxA9/tzrEqwnHzRf0Z8tnOXTosof7Jq9g7bJcFsxq2/SOGSQ7u5oBfYp5YPLJLF7VnRsveZf/95V5DB1QxK2/Hd/gficPXc2C5T1aZJO3RrrX6hIR+hWY2TU109pt3hJL+fmq9sA93+nLmV/byqnjtwHQteceThm/DTM45vidZGXBtpJsuvXcw7EnldOhS4y8Ns6JZ25n2cf5tcdavjCPWIzapnSUbPksB4BtW3J45+UOHHP8zpAjOvQ2by1g89YCFq+KtwjenNePgX2K6dmljEfveIbJ9z5Jt07l/OmnU+jc/vPfZ+yI5bw2+6iwwj5o8Xl7sxJa0lkqo2tspqVa7j7B3Ue4+4huXVJ7E9Ud7r+lD4UDdnHRtZtry08et40P34nXWtYtz2XPbqND5xgnnFHGqsV5VO40YlXw0Xtt6TNwV+1+bzzXiTPOL01pzOkoNz9GfkGsdv2E08tY9UnLrcUcqJLtbdi8tYDCHqUADD9mA5+u6coFt13JZT+5nMt+cjmbtxbw3f/4GiXb2wDBfcKBn/H2h0eEGPnBSmzO3uYY6j6VUtnsrZ1RiXjSuwz4egrP16SFswt47f860+8LFfzrWfGb1N/60QbOuayE+28u5JoxR5OT49z2wBrMoF3HGF+7djM3jh+IGYw8czujztpee7yZL3Tknr+sCOtyQtOpWxV3ProKiPegv/5sJ+a+0T7coELywORT+OnVr5OTXc2G4nbcN+n0Rrc/7fhVzFnUi8rdOYcowuYXn7qy5ff2mjfUHdUcBzcbD/yWz2dU2m/ikbpGDM3z2a8UNrZJpJ1z+LCwQ0h7lV8ZGXYIaW3+mw9QVrruoKpkvQZ39O89dWpC2/50yEsfNDVpeVhS+pDWgcyoJCLpTw85i0jkxMfzS+/7eYlQ8hORJGkkZxGJoPijLqr5iUjEZMq7vUp+IpI0DWklIpETH9JKzV4RiSDd8xORyImP6qJmr4hETPz1NiU/EYmczKj5tfwrEJFDrhpLaGmMmRWa2etmtsjMFprZTUF5ZzObYWZLg7+dgnIzswfNbJmZfWRmw+sc66pg+6VmdlUi16DkJyJJqentTWRpQhVwi7sPAk4Crg+murgdeM3dBwCvBZ8hPiXGgGC5BngY4skSuJP4YMkjgTtrEmZjlPxEJGnNMZipuxe5+7xgvQxYTHwE+POBScFmk4ALgvXzgcc97n2go5n1BM4BZrh7ibtvBWYA45q6Bt3zE5Gk1MzhkaCuZja3zucJ7j5h343MrC9wPDAL6OHuRcFXnwE9gvX6psbo1Uh5o5T8RCQpDlQl3uFR3NR4fmbWFngG+IG7b7c6s4G5u5tZSgYdVbNXRJLWXHN4mFkO8cT3hLtPCYo3Bs1Zgr+bgvKGpsZIaMqMfSn5iUhyEpy2sqmmscWreI8Ci939/jpfTQVqemyvAp6vU/7NoNf3JGBb0Dx+BTjbzDoFHR1nB2WNUrNXRJLSjIOZngJcCXxsZvODsh8D9wFPmdnVwGrgkuC7acB4YBmwE/gWgLuXmNk9xOcNArjb3UuaOrmSn4gkrTne7XX3t6HBLDq2nu0duL6BY00EJiZzfiU/EUmKBjMVkUhyjKrqlt9doOQnIknTBEYiEj2uZq+IRJDu+YlIZCn5iUjkOEZMHR4iEkXq8BCRyHF1eIhIVLmSn4hET1Lj+aUtJT8RSZpqfs1sweZufOGP3ws7jLTVh3fDDiH9pWTYS6nLHWLVSn4iEkHq7RWRyHHU7BWRSFKHh4hElGfAvVUlPxFJmpq9IhI58d5evdsrIhGkZq+IRJKavSISOY4p+YlINGVAq1fJT0SS5OB6vU1EokjNXhGJpIzu7TWz39FI097dv5+SiEQkrUXh3d65hywKEWk5HMjk5Ofuk+p+NrM27r4z9SGJSLrLhGZvk++omNkXzWwR8EnweaiZPZTyyEQkTRlendiSzhJ5Qe+3wDnAFgB3/xAYncKYRCTdeYJLGkvo7WR3X7tPUSwFsYhIS+DxDo9ElqaY2UQz22RmC+qU/dzM1pvZ/GAZX+e7H5nZMjNbYmbn1CkfF5QtM7PbE7mMRJLfWjM7GXAzyzGzW4HFiRxcRDJU89X8HgPG1VP+G3cfFizTAMxsEHAZMDjY5yEzyzazbOB/gHOBQcDlwbaNSiT5XQdcD/QCNgDDgs8iElmW4NI4d58JlCR40vOBye6+y91XAsuAkcGyzN1XuPtuYHKwbaOafMjZ3YuBKxIMTkSioDrlZ7jBzL5J/JG7W9x9K/EK2Pt1tlkXlAGs3ad8VFMnSKS390gze8HMNgdt8+fN7MiEL0FEMkvNc36JLNDVzObWWa5J4AwPA0cRb2UWAb9OxWUk8nrb34i3py8MPl8GPEkCmVVEMlMSz/kVu/uI5I7tG2vWzexPwIvBx/VAYZ1NewdlNFLeoETu+bVx97+4e1Ww/BXIS2A/EclUKXzUxcx61vl4IVDTEzwVuMzMcs2sHzAAmA3MAQaYWT8za028gja1qfM09m5v52D170HX8WTil3MpMC3J6xGRTNJMr7eZ2ZPAGcSbx+uAO4EzzGwY8XyzCrgWwN0XmtlTwCKgCrje3WPBcW4AXgGygYnuvrCpczfW7P0gOHnNVV5b5zsHfpTY5YlIprFmeoDZ3S+vp/jRRra/F7i3nvJpJFkpa+zd3n7JHEhEIsIN0vzVtUQkNJ6fmQ0h/vBg7b0+d388VUGJSJpL81fXEtFk8jOzO4m3yQcRr1aeC7wNKPmJRFUGJL9Eenv/BRgLfObu3wKGAh1SGpWIpLcMGNggkWZvhbtXm1mVmbUHNrH3MzUtxn+c/jpnHLGKkop8vvr0ZQCcc+RybjhhDkd22solUy5iYXH32u0Hdt7CXaPfpG3ObqrduPjZi8gy57dnTaew/Xaq3Xh9dV/un31SWJcUim6H7+a2B9bQsVsVOEz7axeee7Rb2GGFom3+Lm775lv061UCbvzXpNGMGrKWU4etptqhtCyfX/z5dLZsK2DYwA3ce/10iorbAfDWvH5Meml4yFdwADJ9MNM65ppZR+BPxHuAdwDvNbWTmU0EzgM2ufuQgwmyuTz36dH8beEQ7hvzWm3Z0pLO3Dj9HO4aPXOvbbOtml+e+Sr//o+xLCnpSsfcSqqqs2idHWPiR8OYvaEXOVkxJp43ldMKV/PW2iMO9eWEJlZlTLj7cJZ93Ib8ghi/f/lT5s1sx5ql0Xv888ZL32P2wt7c+cezaJUdI691FSs3dGLi1PhzvReduYCrzpvH/U+cBsBHSw/jR7+v7z3+lqW5envD1GSz192/5+6l7v4H4EvAVUHztymPUf9oDaGZW3Q4pZW5e5WtKO3Eqm2d9tv2lN5rWVLShSUlXQEo3ZVHtWdRWZXD7A3x1wn3VGezqLgbhxWUpz74NFKyKYdlH7cBoKI8m7XL8ujac0/IUR16Bfm7GTqwiJfePhqAqlg2Oypy2VnZunabvNZVGVFL2k8mN3vNrMH6uJkNd/d5jR3Y3WeaWd+DiC1UfTuWght/Gv8infMqmLa8P49+ePxe27RrvYsxR6ziLx8fF06QaaBH790cNaSCT+a1CTuUQ65nlzJKy/K5/f+9Sf/eJSxZ3ZXf/e8Xqdydw3cumMM5Jy1lR0VrfvDrL9fuM/jITTx6xzNs2daGh54exaqizo2cIX1lQs2vsWZvYy8TO3BmcwQQvOh8DUCrDvvXwMKSbc7ww4q4+NmLqKxqxZ/Pe4GFxd14f33v4PtqfjV2Bn9dcCzrytqHHG048trEuOORVfzhZ4ezc0d22OEcctnZ1QzoU8wDk09m8cru3Hjpu3x93IdMnDqCR547kUeeO5Erxs3na2MW8ecXTuDTNV259EeXU7Erh1FD1nDv92ZwxR2Xhn0ZByYDarMNNnvdfUwjS7MkvuA8E9x9hLuPyG5T0FyHPWgbywuYW9ST0sp8KqtymLmmD4O6bq79/q7Rb7J6W0ce/3hoiFGGJ7uVc8cjq/jHlE688/eOYYcTis1bC9i8tYDFK+OdZG9+0I+BRxTvtc2M2f0ZPXwlADsrW1OxKweAWQv6kJ1dTYe2lYc26OaQaJM3zWuHCQ1jH0Vvr+3DwM4l5LXaQ7ZVc2LPDSzfGm+i3HTiLNq13sUv3j0l5CjD4tz867WsXZrHlAnR7OUFKNnehs1bCyjsUQrA8C9sYNWGTvTqvq12m1OHrmLNZx0B6Nx+JzUZ4Zi+m8jKcrbtyKVFyoDkl9AbHpniV2NnMLLnBjrmVfL6FY/z+7knsm1XLj855W0651fwh3On8cmWrnx32nls353LYx8P5ekLn8GBmWuO4M01R9CjYAfXDZ/H8q0deeaipwH428Ih/N8nTY6anTEGjyznrIu3smJRHg/NWALAn3/Rkzn/iF7z/4EnT+GnV79OTqtqNhS3477HTueH35xJYY9tuBsbt7Tl10+cCsDpJ6zk/NMXEYtlsWtPK+6aMJZERjtOR5b6wUxTzjxFE3DWHa0B2Ajc6e4NvrAMkHd4oR9x7c0piScT9Lnr3bBDSHuV540MO4S0Nn/mA5SVrjuojJtbWOi9b/q3hLZdcdstHyQ7nt+hksjrbUZ8GPsj3f1uM+sDHObusxvbr4HRGkSkhTPPjN7eRO75PQR8EahJZmXER3YWkahKfBj7tJXIPb9R7j7czP4J4O5bg9FSRSSqMqDml0jy2xPMi+kAZtaNQzF3k4ikrUxo9iaS/B4EngW6m9m9xEd5+WlKoxKR9OWZ0dubyLy9T5jZB8SHtTLgAndfnPLIRCR9RaHmF/Tu7gReqFvm7mtSGZiIpLEoJD/gJT6fyCgP6AcsAQanMC4RSWORuOfn7sfW/RyM9vK9lEUkInIIJP16m7vPM7NRqQhGRFqIKNT8zKzu+2ZZwHBgQ8oiEpH0FpXeXqBdnfUq4vcAn0lNOCLSImR6zS94uLmdu996iOIRkTRnZHiHh5m1cvcqM4vqoHUi0pBMTn7AbOL39+ab2VTgaaB2ph53n5Li2EQkHWXIqC6J3PPLA7YQn7Oj5nk/B5T8RKIqwzs8ugc9vQv4POnVyIC8LyIHKtNrftlAW+ofZzsDLl1EDlgGZIDGkl+Ru999yCIRkZahBUxOlIjGRnJO72FYRSQ0NUPZN7U0eRyziWa2ycwW1CnrbGYzzGxp8LdTUG5m9qCZLTOzj4JXbWv2uSrYfqmZXZXINTSW/MYmcgARiaDmm7ryMWDcPmW3A6+5+wDgteAzwLnAgGC5BngY4skSuBMYBYwE7qxJmI1pbNLykoRCF5HIserElqa4+0xg31xzPjApWJ8EXFCn/HGPex/oaGY9gXOAGe5e4u5bgRnsn1D3E6l5e0WkGSR3z6+rmc2t83mCu09oYp8e7l4UrH8G9AjWewFr62y3LihrqLxRSn4ikhQjqQ6B4oOZt9fd3Sw1D9YkMnWliMjemu+eX302Bs1Zgr+bgvL1QGGd7XoHZQ2VN0rJT0SS1ly9vQ2YCtT02F4FPF+n/JtBr+9JwLagefwKcLaZdQo6Os4OyhqlZq+IJK+ZGqJm9iRwBvF7g+uI99reBzxlZlcDq4FLgs2nAeOBZcTnFfoWxDtnzeweYE6w3d2JdNgq+YlIcppxMFN3v7yBr/Z71M7dHbi+geNMBCYmc24lPxFJXga84aHkJyJJy/SBDURE6qfk17ysGlpvDzsKacmKTs0OO4S0tmde87yyr5qfiESPk/GDmYqI7CfjJzASEWmQkp+IRJF5y89+Sn4ikpwMGclZyU9EkqZ7fiISSc31eluYlPxEJHmq+YlI5BzccFVpQ8lPRJKn5CciUaOHnEUksqy65Wc/JT8RSY6e8xORqNKjLiISTar5iUgUqcNDRKLHAQ1sICJRpHt+IhI5es5PRKLJXc1eEYkm1fxEJJqU/EQkilTzE5HocSDW8rOfkp+IJE01PxGJJvX2ikgUqeYnItGTIUNaZYUdgIi0LAZYzBNamjyW2Soz+9jM5pvZ3KCss5nNMLOlwd9OQbmZ2YNmtszMPjKz4QdzHUp+IpI0c09oSdAYdx/m7iOCz7cDr7n7AOC14DPAucCAYLkGePhgrkHJT0SS40ksB+Z8YFKwPgm4oE754x73PtDRzHoe6Ekidc/vrnGvM/rIVZTszOeixy4DoH1eJb/8ygwO71DGhm3tuG3q2ZTtygWcfz/zHU49cjWVVa24Y9qZfLKpGwA/GP0epx25GoAJ743glSX9w7qk0EyatYiKHdlUV0Osyrjx3IFhh3RI/OfJrzOm12q2VOZz3guXAvDDE97jzN6r2V2dxdqy9tz+zhjK9uTSq2A7fz//f1m5vSMA8zf34M5ZowH4y9nP0y1/J7ti8f8LfuvV8yipzA/lmpKX1Lu9XWuas4EJ7j5h74Mx3cwc+GPwXQ93Lwq+/wzoEaz3AtbW2XddUFbEAUhZ8jOzQuBx4oE78Yt+IFXnS8TzC47myXlDuHf8a7Vl3x71T2av7sXE2cP59sh5XD1qHr+d+UVO7beGPp1K+cojX+fYnhv56Zdm8o0nLuK0I1dzTI9iLpl0Ca1bxXjk0ud5e2Ufyne3DvHKwvHDi49ie0mk/vvJlGVH89dPhvDLU/5RW/bOht78et4oYp7FrcPf59pj/8mv5p0EwJqy9pz/4sX1HuvWt8eyYEv3QxJ3c0uit7e4TnO2Pqe6+3oz6w7MMLNP6n7p7h4kxmaXymZvFXCLuw8CTgKuN7NBKTxfk+atO5ztlbl7lY3pv5KpC48GYOrCoxkzYGW8fMAqXlh4NGB8XHQY7fJ20bWgnCO7lDBvXU9inkXFnhyWbu7CKf3WHOpLkZDM3XQ423bt/W/onaJCYh7/v9KHm3twWJsdYYR2aNWM7NLU0uRhfH3wdxPwLDAS2FjTnA3+bgo2Xw8U1tm9d1B2QFKW/Ny9yN3nBetlwGLiVdS00rlNBcXlBQAUl7ehc5sKALq3LWdjWdva7TaWtaV723I+3dyVk/utJa/VHjrmV3Bin/Uc1i4C/9j35cZ/PrmC37/8KedesSXsaNLGRf0/Yeb6PrWfe7ct47nznuavZz/PiO57t85+cfIbPH/e03zv2A9oUc+OePP09ppZgZm1q1kHzgYWAFOBq4LNrgKeD9anAt8Men1PArbVaR4n7ZC0WcysL3A8MOtQnO/AWZNbvLeqkMGHbWLSFc+ydWc+H244jJg3vV+mufmC/mz5LIcOXfZw3+QVrF2Wy4JZbZveMYNdd+wHxNyYunIAAJsqCjhjyjco3ZXH4M6beWjMy4yfeinle1pz61tj2VjRloJWu/ndGdO5oPxTnltxdMhXkITmydU9gGfNDOK56G/u/rKZzQGeMrOrgdXAJcH204DxwDJgJ/Ctgzl5ypOfmbUFngF+4O7b6/n+GuLd1uS065TqcPZTsjOfrgXlFJcX0LWgnJKd8ZvOm3YU0KNOja5Hux1s2hGvIT7y/gk88v4JAPziyzNYXdLxkMcdti2f5QCwbUsO77zcgWOO3xnp5HfhUZ8wpvcarpp+HjX/Ed1TnU3prmwAFpZ0Y01Ze/q1L2XBlu5srIj/VuVVrXlhZX+O67qpRSW/JB5jaZC7rwCG1lO+BRhbT7kD1x/0iQMpfdTFzHKIJ74n3H1Kfdu4+wR3H+HuI7LbFKQynHq9sawvXx28BICvDl7C68v61ZZ/ZfASwDm252fs2JVLcXkBWVZNh7xKAAZ028LAblt4b1VhQ4fPSLn5MfILYrXrJ5xexqpP8kKOKjynHb6G7w7+kOv+MY7KWE5teafcCrKCyS4K226nb/ttrC1rT7ZV0yk3fnullcUY03sNn5Z2DiX2A9ZM9/zClMreXgMeBRa7+/2pOk8y7jtvBiMKN9Axv5Lp1z3Ow++cyMRZw/nvr07nguM+oWh7W26bejYAb63ow6lHrubF7/6Nyj2t+NnfxwDQKquaP1/+HADlu3P48bSzam92R0WnblXc+egqALJbOa8/24m5b7QPN6hD5P7TXmVkjw10yqtk5kV/4cEPR3DtkH/SOjvGY196Efj8kZYTexRx07A5VFVnUe3Gz94fzbbdeeS32sOjZ71Eq6xqss15t6gXTy39QshXlgQHMmACI/MUZWczOxV4C/iYz3+qH7v7tIb2yT+s0PtfcXNK4skEh/3m3bBDSHsr7/ti2CGktXUP/obKdWsP6iZ1h4LD/aRB1ya07fS5P/+giUddQpOymp+7v00iPQgi0vJUt/yqX7SeUBWRg5chzV4lPxFJWnP09oZNyU9EkqfkJyLRk/6PsSRCyU9EkqPZ20QkqnTPT0SiSclPRCLHgWolPxGJHHV4iEhUKfmJSOQ4EGv5r3go+YlIkhxcyU9EokjNXhGJHPX2ikhkqeYnIpGk5CcikeMOsVjYURw0JT8RSZ5qfiISSUp+IhI9rt5eEYkgB9dDziISSXq9TUQix11TV4pIRKnDQ0SiyFXzE5Ho0WCmIhJFGthARKLIAdfrbSISOa7BTEUkolzNXhGJpAyo+ZmnUa+NmW0GVocdRx1dgeKwg0hj+n2alm6/0RHu3u1gDmBmLxO/rkQUu/u4gzlfqqRV8ks3ZjbX3UeEHUe60u/TNP1G6Ssr7ABERMKg5CcikaTk17gJYQeQ5vT7NE2/UZrSPT8RiSTV/EQkkpT8RCSSlPzqYWbjzGyJmS0zs9vDjifdmNlEM9tkZgvCjiUdmVmhmb1uZovMbKGZ3RR2TLI/3fPbh5llA58CXwLWAXOAy919UaiBpREzGw3sAB539yFhx5NuzKwn0NPd55lZO+AD4AL9G0ovqvntbySwzN1XuPtuYDJwfsgxpRV3nwmUhB1HunL3InefF6yXAYuBXuFGJftS8ttfL2Btnc/r0D9cOUBm1hc4HpgVciiyDyU/kRQxs7bAM8AP3H172PHI3pT89rceKKzzuXdQJpIwM8shnviecPcpYccj+1Py298cYICZ9TOz1sBlwNSQY5IWxMwMeBRY7O73hx2P1E/Jbx/uXgXcALxC/Eb1U+6+MNyo0ouZPQm8BxxtZuvM7OqwY0ozpwBXAmea2fxgGR92ULI3PeoiIpGkmp+IRJKSn4hEkpKfiESSkp+IRJKSn4hEkpJfC2JmseCxiQVm9rSZtTmIYz1mZv8SrD9iZoMa2fYMMzv5AM6xysz2m+WrofJ9ttmR5Ll+bma3JhujRJeSX8tS4e7DgpFUdgPX1f3SzA5oHmZ3/04TI46cASSd/ETSmZJfy/UW0D+olb1lZlOBRWaWbWb/bWZzzOwjM7sW4m8dmNnvg3EKXwW61xzIzN4wsxHB+jgzm2dmH5rZa8GL+dcB/xbUOk8zs25m9kxwjjlmdkqwbxczmx6MYfcIYE1dhJk9Z2YfBPtcs893vwnKXzOzbkHZUWb2crDPW2Z2TLP8mhI5B1RTkHAFNbxzgZeDouHAEHdfGSSQbe5+opnlAu+Y2XTiI4scDQwCegCLgIn7HLcb8CdgdHCszu5eYmZ/AHa4+6+C7f4G/Mbd3zazPsTfhvkCcCfwtrvfbWZfBhJ58+PbwTnygTlm9oy7bwEKgLnu/m9m9rPg2DcQnxDoOndfamajgIeAMw/gZ5SIU/JrWfLNbH6w/hbx90dPBma7+8qg/GzguJr7eUAHYAAwGnjS3WPABjP7Rz3HPwmYWXMsd29ozL6zgEHxV1gBaB+MYDIa+Fqw70tmtjWBa/q+mV0YrBcGsW4BqoH/Dcr/CkwJznEy8HSdc+cmcA6R/Sj5tSwV7j6sbkGQBMrrFgE3uvsr+2zXnO+WZgEnuXtlPbEkzMzOIJ5Iv+juO83sDSCvgc09OG/pvr+ByIHQPb/M8wrwr8GQSpjZQDMrAGYClwb3BHsCY+rZ931gtJn1C/btHJSXAe3qbDcduLHmg5kNC1ZnAl8Pys4FOjURawdga5D4jiFe86yRBdTUXr9OvDm9HVhpZhcH5zAzG9rEOUTqpeSXeR4hfj9vnsUnGPoj8Rr+s8DS4LvHiY/Kshd33wxcQ7yJ+SGfNztfAC6s6fAAvg+MCDpUFvF5r/NdxJPnQuLN3zVNxPoy0MrMFgP3EU++NcqBkcE1nAncHZRfAVwdxLcQTTEgB0ijuohIJKnmJyKRpOQnIpGk5CcikaTkJyKRpOQnIpGk5CcikaTkJyKR9P8B7E7YLXIzq9YAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_pred = model_noFT.predict([XH_test,XA_test,Xplayers_test]).argmax(axis=-1)\n",
    "cm_metrics(y_test,y_test_pred, [0,1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essais avec fine tuning\n",
    "\n",
    "## 1\n",
    "<b>full trainable mais on garde que la sortie des LSTM (concat: LSTM_model.layers[-7])</b>\n",
    "sinon archi du 7e essai:  \n",
    "\n",
    "home_input = tf.keras.Input(shape = (10,6), name = 'home_train_input')  \n",
    "away_input = tf.keras.Input(shape = (10,6), name = 'away_train_input')  \n",
    "LSTM_output = base_LSTM_model([home_input,away_input])  \n",
    "LSTM_output = tf.keras.layers.BatchNormalization()(LSTM_output)\n",
    "\n",
    "\n",
    "player_input = tf.keras.Input(shape = (198), name = 'player_train_input')  \n",
    "concat_layer = tf.keras.layers.Concatenate(name='concat_layer')([LSTM_output,player_input])  \n",
    "Dense1 = tf.keras.layers.Dense(256,'relu',name = 'Dense1')(concat_layer)  \n",
    "Dense1 = tf.keras.layers.Dropout(0.234375)(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dense(128,'relu',name='Dense2')(Dense1)  \n",
    "Dense2 = tf.keras.layers.Dropout(0.125)(Dense2)  \n",
    "Dense_output = tf.keras.layers.Dense(3, 'softmax', name='output')(Dense2)   \n",
    "\n",
    "    Overfitting de dingue. on arrive rapidement à une train_acc = 80% pour une val_acc = 45% (baseline)\n",
    "    trop de Dense layers? essayons avec moins\n",
    "\n",
    "## 2 \n",
    "<b>full trainable mais on garde que la sortie des LSTM après dropout (LSTM_model.layers[-6])  \n",
    "On retire des layers Dense dans l'architecture pour éviter l'overfitting</b>\n",
    "\n",
    "    Meme problème, aucune différence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7487845996c7ba5663e8475bda763caf28d03a7733caf66b680b36d8ed7e8fd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('foot_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
